\section{Conclusion}

We presented an empirical study on knowledge grounding in LLMs, probing how models respond when provided with contradictory context. We showed that encoder-decoder architectures and smaller models better integrate new evidence, while large decoder-only models often revert to their \Parametric{} knowledge. We also demonstrated that perplexity can serve as a useful indicator to detect potential hallucinations and guide adaptive retrieval strategies.

These insights can inform the selection of models and inference strategies for tasks where factual accuracy is crucial. Future work includes improving answer equivalence checks, exploring more complex contradictions, and fine-tuning models specifically for robust retrieval-augmented reasoning. By deepening our understanding of knowledge grounding, we take a step closer to building more trustworthy and reliable language models.
