Large language models have become integral tools for a wide range of NLP tasks.
While hallucinations remain a significant challenge when factual accuracy is crucial, RAG mitigates some issues by providing external context.
However, it is unclear whether the model will rely on the retrieved evidence or on its internal knowledge.

This paper conducts an empirical study of \emph{knowledge grounding} in LLMs.
We develop a diverse dataset of short-answer questions and present them to two encoder-decoder models (\smallflan{}, \bigflan{}) and two decoder-only models (\smallllama{}, \bigllama{}) along with with added counterparametric answers in their context.
We find that encoder-decoder models and smaller models lean more on the given context, while larger decoder-only models often ignore contradictions and rely on parametric knowledge. We also demonstrate that perplexity correlates with whether an answer is sourced from parametric memory or contextual evidence, suggesting that perplexity can serve as a useful signal for assessing the reliability of an answer and the modelâ€™s reliance on contextual evidence versus parametric memory.
% a practical tool for detecting when re-retrieval or other interventions may be needed.

% Our findings have implications for building more reliable and grounded LLM-based systems and guide future research in mitigating hallucinations.
