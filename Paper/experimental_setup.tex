\section{Experimental Setup}

We design controlled experiments to test how LLMs handle contradictory context. We first gather parametric answers from each model for a set of questions, then add counterparametric context and re-ask the questions.

\textbf{Dataset:}
We create a large, diverse dataset of short-answer questions spanning several domains: historical figures, cities, scientific principles, elements, books, paintings, events, buildings, and musical compositions. These questions have known, short, and unambiguous answers (e.g., “What country is Cairo in?”).

Following \citet{factual_recall}, we inject counterparametric context by taking an answer from one object and using it as contradictory context for another. For example, if the model originally answered “Cairo is in Egypt,” we provide context stating “Cairo is in India” and re-ask the question. This setup tests whether the model chooses the \Contextual{} or \Parametric{} answer, or produces \Other{} outputs.

\textbf{Models:}
We evaluate four LLMs of different architectures and sizes:

\begin{table}[t]
\centering
\small
\begin{tabular}{l l r}
\toprule
Model & Architecture & \#Parameters \\
\midrule
Flan-T5-XL   & Encoder-Decoder & 3B \\
Flan-T5-XXL  & Encoder-Decoder & 11B \\
Meta-Llama-3.1-8B-Instruct   & Decoder-Only & 8B \\
Meta-Llama-3.1-70B-Instruct  & Decoder-Only & 70B \\
\bottomrule
\end{tabular}
\caption{Models evaluated in this study. Flan-T5 variants are Seq2Seq models; Llama variants are decoder-only.}
\label{tab:models}
\end{table}

Flan-T5 \citep{flant5} is an instruction-tuned T5 model \citep{t5} with strong zero-shot capabilities. Llama \citep{llama3} is a decoder-only architecture fine-tuned for instructions. Together, these models allow us to contrast encoder-decoder vs.\ decoder-only and small vs.\ large architectures.

\textbf{Procedure:}
1. We query each model without context to obtain its \Parametric{} answer.
2. We sample a counterparametric answer from another query-object pair and inject it as context.
3. We re-query the model with this contradictory context and categorize the new answer as \Parametric{}, \Contextual{}, or \Other{}.

We use greedy decoding for consistency. Though more sophisticated decoding methods exist, short-answer tasks are less sensitive to decoding strategy \citep{t5}.

\textbf{Perplexity Calculation:}
To understand the model’s internal confidence, we use teacher-forcing to compute perplexities for both \Parametric{} and \Contextual{} answers under the original and contradictory queries. Higher perplexity suggests the model finds the sequence less probable, offering a clue to whether an answer stems from parametric memory or from the provided context.

\textbf{Computational Resources:}
All experiments were run on a server equipped with dual NVIDIA A100 GPUs (80GB VRAM each) and 48 CPU cores. The A100’s large memory footprint allowed us to load and run the largest (70B) model efficiently. This high-performance hardware ensured that both inference and perplexity computation could be completed in a reasonable time frame.

