@misc{survey_hallucinations,
	title = {{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}},
	author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
	year={2023},
	eprint={2311.05232},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{hallucination_is_inevitable,
	title = {{Hallucination is Inevitable: An Innate Limitation of Large Language Models}},
	author={Ziwei Xu and Sanjay Jain and Mohan Kankanhalli},
	year={2024},
	eprint={2401.11817},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{atlas_foundational,
	title = {{Atlas: Few-shot Learning with Retrieval Augmented Language Models}},
	author={Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
	year={2022},
	eprint={2208.03299},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{shall_we_pretrain_autoregressive,
	title = {{Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study}},
	author = {Wang, Boxin  and Ping, Wei  and Xu, Peng  and McAfee, Lawrence  and Liu, Zihan  and Shoeybi, Mohammad  and Dong, Yi  and Kuchaiev, Oleksii  and Li, Bo  and Xiao, Chaowei  and Anandkumar, Anima  and Catanzaro, Bryan},
	editor = {Bouamor, Houda  and Pino, Juan  and Bali, Kalika},
	booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	month = dec,
	year = {2023},
	address = {Singapore},
	publisher = {Association for Computational Linguistics},
	url = {\url{https://aclanthology.org/2023.emnlp-main.482}},
	doi = {10.18653/v1/2023.emnlp-main.482},
	pages = {7763--7786},
}

@misc{retro,
	title = {{Improving language models by retrieving from trillions of tokens}},
	author={Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and Tom Hennigan and Saffron Huang and Loren Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and Karen Simonyan and Jack W. Rae and Erich Elsen and Laurent Sifre},
	year={2022},
	eprint={2112.04426},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{selfrag,
	title = {{Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}},
	author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
	year={2023},
	booktitle={International Conference on Learning Representations},
	year={2024}
}

@misc{factual_recall,
	title = {{Characterizing Mechanisms for Factual Recall in Language Models}},
	author={Qinan Yu and Jack Merullo and Ellie Pavlick},
	year={2023},
	eprint={2310.15910},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/2310.15910}},
}

@article{fewshotlearners,
	title = {{Language models are few-shot learners}},
	author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={arXiv preprint arXiv:2005.14165},
	year={2020}
}

@inproceedings{beyondfewshot,
	title = {{Prompt programming for large language models: Beyond the few-shot paradigm}},
	author={Reynolds, Laria and McDonell, Kyle},
	booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
	pages={1--7},
	year={2021}
}

@article{factscore,
	title = {{FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation}},
	author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	journal={arXiv preprint arXiv:2305.14251},
	year={2023}
}

@inproceedings{dora,
	title = {{DoRA: Weight-Decomposed Low-Rank Adaptation}},
	author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
	booktitle={International Conference on Learning Representations},
	year={2023}
}

@article{passage_retrieval,
	title = {{Leveraging passage retrieval with generative models for open domain question answering}},
	author={Izacard, Gautier and Grave, Edouard},
	journal={arXiv preprint arXiv:2007.01282},
	year={2020}
}

@article{rethinking_demonstrations,
	title = {{Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}},
	author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	journal={arXiv preprint arXiv:2202.12837},
	year={2022}
}
@article{how_many_data_points,
	title = {{How Many Data Points is a Prompt Worth?}},
	author={Le Scao, Teven and Rush, Alexander M.},
	journal={arXiv preprint arXiv:2103.08493},
	year={2021}
}
@article{knowledge_grounding_retrieval_augmented,
	title = {{Knowledge Grounding in Retrieval-Augmented LM: An Empirical Study}},
	author={Whitehouse, Chenxi and Chamoun, Eric and Aly, Rami},
	journal={arXiv preprint},
	year={2023}
}
@article{large_models_struggle_long_tail,
	title = {{Large Language Models Struggle to Learn Long-Tail Knowledge}},
	author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
	journal={Proceedings of the 40th International Conference on Machine Learning},
	year={2023}
}

@misc{pretrainpromptpredict,
	title = {{Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}},
	author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
	year={2021},
	eprint={2107.13586},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/2107.13586}},
}

@article{lora,
	title = {{LoRA: Low-Rank Adaptation of Large Language Models}},
	author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	journal={arXiv preprint arXiv:2106.09685},
	year={2021}
}

@article{multiclass_adaptation,
	title = {{Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures}},
	author={Lin, Chu-Cheng and Wang, Xinyi and Clark, Jonathan H. and Lu, Han and Zhu, Yun and Whitehouse, Chenxi and Yu, Hongkun},
	journal={arXiv preprint arXiv:2402.17934},
	year={2024}
}

@inproceedings{parameter_efficient_multilingual_summarization,
	title = {{Parameter-Efficient Multilingual Summarisation: An Empirical Study}},
	author={Whitehouse, Chenxi and Huot, Fantine and Bastings, Jasmijn and Dehghani, Mostafa and Lin, Chu-Cheng and Lapata, Mirella},
	booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	pages={7763--7786},
	year={2023}
}

@article{ragged,
	title = {{RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems}},
	author={Hsia, Jennifer and Shaikh, Afreen and Wang, Zhiruo and Neubig, Graham},
	journal={arXiv preprint arXiv:2403.09040},
	year={2024}
}

@article{rag,
	title = {{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}},
	author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={9459--9474},
	year={2020}
}

@article{surface_based_retrieval,
	title = {{Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models}},
	author={Doostmohammadi, Ehsan and Norlund, Tobias and Kuhlmann, Marco and Johansson, Richard},
	journal={arXiv preprint arXiv:2305.16243},
	year={2023}
}

@article{true_teacher,
	title = {{TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models}},
	author={Gekhman, Zorik and Herzig, Jonathan and Aharoni, Roee and Elkind, Chen and Szpektor, Idan},
	journal={arXiv preprint arXiv:2305.11171},
	year={2023}
}

@article{t5,
	title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
	author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	journal={Journal of Machine Learning Research},
	volume={21},
	pages={1--67},
	year={2020}
}

@inproceedings{how_can_we_know,
	title = {{How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering}},
	author={Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
	booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	pages={1974--1991},
	year={2021},
	publisher={Association for Computational Linguistics},
	url={\url{https://aclanthology.org/2021.emnlp-main.150}}
}

@misc{sutskever_seq2seqlearning,
	title = {{Sequence to Sequence Learning with Neural Networks}},
	author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
	year={2014},
	eprint={1409.3215},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/1409.3215}},
}

@misc{wu_mltranslation,
	title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
	author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and ≈Åukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
	year={2016},
	eprint={1609.08144},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/1609.08144}},
}

@article{gpt2,
	title = {{Language models are unsupervised multitask learners}},
	author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	journal={OpenAI blog},
	volume={1},
	number={8},
	pages={9},
	year={2019}
}

@inproceedings{realm,
	title = {{REALM: Retrieval-Augmented Language Model Pre-Training}},
	author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	booktitle={International Conference on Machine Learning},
	pages={3929--3938},
	year={2020},
	organization={PMLR}
}

@article{text_degeneration,
	title = {{The curious case of neural text degeneration}},
	author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	journal={arXiv preprint arXiv:1904.09751},
	year={2020}
}

@inproceedings{teacher_forcing,
  title = {{Professor Forcing: A New Algorithm for Training Recurrent Networks}},
  author={Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016},
  publisher={Curran Associates, Inc.}
}

@misc{disco,
	title = {{DISCO: Distilling Counterfactuals with Large Language Models}},
	author={Zeming Chen and Qiyue Gao and Antoine Bosselut and Ashish Sabharwal and Kyle Richardson},
	year={2023},
	eprint={2212.10534},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/2212.10534}},
}

@misc{learning_the_difference,
	title = {{Learning the Difference that Makes a Difference with Counterfactually-Augmented Data}},
	author={Divyansh Kaushik and Eduard Hovy and Zachary C. Lipton},
	year={2020},
	eprint={1909.12434},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/1909.12434}},
}

@misc{disentqa,
	title = {{DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering}},
	author={Ella Neeman and Roee Aharoni and Or Honovich and Leshem Choshen and Idan Szpektor and Omri Abend},
	year={2022},
	eprint={2211.05655},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/2211.05655}},
}

@misc{when_not_to_trust_llms,
	title = {{When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories}},
	author={Alex Mallen and Akari Asai and Victor Zhong and Rajarshi Das and Daniel Khashabi and Hannaneh Hajishirzi},
	year={2023},
	eprint={2212.10511},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/2212.10511}},
}

@misc{can_rag_models_reason,
	title = {{Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model}},
	author={Parishad Behnam Ghader and Santiago Miret and Siva Reddy},
	year={2023},
	volume={abs/2212.09146},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={\url{https://arxiv.org/abs/2212.09146}},
}

@article{BehnamGhader2022CanRL,
  title = {{Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model}},
  author={Parishad Behnam Ghader and Santiago Miret and Siva Reddy},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.09146},
  url={\url{https://api.semanticscholar.org/CorpusID:254854344}}
}

@misc{can_we_edit_factual_knowledge,
      title = {{Can We Edit Factual Knowledge by In-Context Learning?}},
      author={Ce Zheng and Lei Li and Qingxiu Dong and Yuxuan Fan and Zhiyong Wu and Jingjing Xu and Baobao Chang},
      year={2023},
      eprint={2305.12740},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={\url{https://arxiv.org/abs/2305.12740}},
}

@inproceedings{kilt,
    title = {{KILT: a Benchmark for Knowledge Intensive Language Tasks}},
    author = {Petroni, Fabio  and Piktus, Aleksandra  and Fan, Angela  and Lewis, Patrick  and Yazdani, Majid  and De Cao, Nicola  and Thorne, James  and Jernite, Yacine  and Karpukhin, Vladimir  and Maillard, Jean  and Plachouras, Vassilis  and Rockt{\"a}schel, Tim  and Riedel, Sebastian},
    editor = {Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    month = jun,
    year = {2021},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {\url{https://aclanthology.org/2021.naacl-main.200}},
    doi = {10.18653/v1/2021.naacl-main.200},
    pages = {2523--2544},
}

@misc{treeofthoughts,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10601},
}

@article{natural_questions,
	title	= {{Natural Questions: a Benchmark for Question Answering Research}},
	author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
	year	= {2019},
	journal	= {Transactions of the Association of Computational Linguistics}
}

@inproceedings{pythia,
	title={Pythia: A suite for analyzing large language models across training and scaling},
	author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O‚ÄôBrien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
	booktitle={International Conference on Machine Learning},
	pages={2397--2430},
	year={2023},
	organization={PMLR}
}

@misc{flant5,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.11416}, 
}

@misc{llama3,
      title={{The Llama 3 Herd of Models}},
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@article{wikipedia_geographic_bias,
	author = {Beyt√≠a, Pablo},
	year = {2020},
	month = {03},
	pages = {},
	title = {The Positioning Matters. Estimating Geographical Bias in the Multilingual Record of Biographies on Wikipedia},
	journal = {SSRN Electronic Journal}
}

@inproceedings{attention_is_all_you_need,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
	title = {Attention is all you need},
	year = {2017},
	isbn = {9781510860964},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	pages = {6000‚Äì6010},
	numpages = {11},
	location = {Long Beach, California, USA},
	series = {NIPS'17}
}


