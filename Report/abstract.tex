\begin{abstract}

In recent years large language models have exploded in quality and prevalence, and they have become crucial for work in a wide range of areas.
However, their tendency to produce hallucinations presents a critical challenge in contexts where precision and correctness are crucial.

Retrieval-Augmented Generation (RAG), which leverages external information to provide more accurate and contextually appropriate responses, has been proposed as a solution to this problem.
However, this solution is far from perfect as it's unclear when a large language model will choose the information provided by the RAG model over the knowledge in its parametric memory.

This thesis explores the \emph{Knowledge Grounding} of various large language models.
In particular, it attempts to answer a simple research question: \textbf{How does a large language model respond when given information that contradicts its inherent knowledge, and why?}

To investigate this, we develop a diverse dataset comprising questions from various topics and globally representative data.
We use this dataset to construct queries with counterparametric context across four models of different architectures and sizes, and later we test models of various architectures and sizes to find out which type of answer they choose.
We also analyse the \emph{perplexity} of these answers to give us a clue to why the model chose an answer over the other, and to make a rough prediction of the source of a particular answer.

Our findings suggest that, when using RAG-enhanced systems, smaller and simpler models and models that encode the entire input in their memory might produce more answers from the RAG-provided context and therefore fewer hallucinations.
In particular, the smaller models \texttt{Meta-Llama-3.1-8B} and \texttt{Flan-T5-XL} tend to have better knowledge grounding and fewer hallucinations than their larger versions, while simpler encoder-decoder Seq2Seq models tend to outperform the more complex Decoder-only models.

As an extra analysis we investigate methods for determining whether a given response originates from the RAG context or the model's internal memory from the query's resulting perplexity, which may indicate a potential hallucination.

This thesis forms the foundational part of a broader project aimed at publishing a comprehensive study on knowledge grounding in retrieval-augmented language models, as outlined in the preprint ``Knowledge Grounding in Retrieval-Augmented LMs: An Empirical Study'' \citep{knowledge_grounding_retrieval_augmented}.
We build on existing literature, incorporating the use of counterparametric context in queries, to advance our understanding of this phenomenon.

\end{abstract}
