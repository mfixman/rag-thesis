\begin{abstract}

In recent years large language models have exploded in quality and prevalence, and they have become crucial for work in a wide range of areas.
However, their tendency to produce hallucinations presents a critical challenge in contexts where precision and correctness are crucial.

Retrieval-Augmented Generation (RAG), which leverages external information to provide more accurate and contextually appropriate responses, has been proposed as a solution to this problem.
However, this solution is far from perfect as it's unclear when a large language model will choose to generate answers using the context provided by RAG over the knowledge in its parametric memory.

This thesis explores the \emph{Knowledge Grounding} of various large language models.
In particular, it attempts to answer a research question: \textbf{How does a large language model respond to questions when context  information contradicts or supports its inherent parametric knowledge?}

To investigate this, we develop a diverse dataset comprising questions on various topics and using diverse data.
We use this dataset to construct queries with counter-parametric context, i.e. the context information contradicts the answer given by the model without context, and parametric context. We test models of different architectures and sizes to find out which type of answer they choose.
We also analyse the \emph{perplexity} of these answers to give us an indication on certainty of the model in its answer, and observe differences in the perplexity between answers that agree or disagree with the context information. 

Our findings suggest that smaller models and models that encode the entire input sequence into an internal representation before outputting an answer might produce more answers sourced from the RAG-provided context, therefore avoiding hallucinations.
In particular, the smaller models \texttt{Meta-Llama-3.1-8B} and \texttt{Flan-T5-XL} tend to have better knowledge grounding and fewer hallucinations than their larger versions, while encoder-decoder Seq2Seq models tend to outperform Decoder-only models.
%As an additional analysis we investigate methods for determining whether a given response originates from the RAG context or the model's internal memory from the query's resulting perplexity.
%This might be used to develop methods to prevent hallucinations in large language models that use RAG indexing.


%This thesis forms the foundational part of a broader project aimed at publishing a comprehensive study on knowledge grounding in retrieval-augmented language models, as outlined in the preprint ``Knowledge Grounding in Retrieval-Augmented LMs: An Empirical Study'' \citep{knowledge_grounding_retrieval_augmented}.
%We build on existing literature, incorporating the use of counterparametric context in queries, to advance our understanding of this phenomenon.

\end{abstract}
