\section{Related Work}

This research is the latest on a long line of academic articles on the topics of retrieval-augmented generation, counterparametric and contextual data, and how to enhance knowledge on large language models.

This section presents a short summary of some of the articles that were useful in researching this topic.

\footnotetext[1]{This entire section is in progress --- short summaries of the named papers will come soon.}

\subsection{Foundational Papers on Large Language Models}
\begin{itemize}
	\item ``Language models are unsupervised multitask learners''\cite{gpt2}.
	\item ``Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer''\cite{t5}.
	\item ``Language Models are Few-shot Learners''\cite{fewshotlearners}.
	\item ``Prompt programming for large language models: Beyond the few-shot paradigm''\cite{beyondfewshot}.
\end{itemize}

\subsection{Papers working with RAG and contextual data}
\begin{itemize}
	\item ``Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks''\cite{rag}.
	\item ``Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection''\cite{selfrag}.
	\item ``Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model''\cite{can_rag_models_reason}.
\end{itemize}

\subsection{Retrieval-Augmented Language Models}
\begin{itemize}
	\item ``Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study''\cite{shall_we_pretrain_autoregressive}.
	\item ``Atlas: Few-shot Learning with Retrieval Augmented Language Models''\cite{atlas_foundational}.
	\item ``Improving language models by retrieving from trillions of tokens''\cite{retro}.
	\item ``RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems''\cite{ragged}.
\end{itemize}

\subsection{On disentangling parametric and context-augmented counterparametric knowledge}
\begin{itemize}
	\item ``DISCO: Distilling Counterfactuals with Large Language Models''\cite{disco}.
	\item ``DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering''\cite{disentqa}.
	\item ``Characterizing Mechanisms for Factual Recall in Language Models''\cite{factual_recall}.
	\item ``Can We Edit Factual Knowledge by In-Context Learning?''\cite{can_we_edit_factual_knowledge}.
	\item ``Learning the Difference that Makes a Difference with Counterfactually-Augmented Data''\cite{learning_the_difference}.
\end{itemize}

