\section{Context}

This research is the latest on a long line of academic articles on retrieval-augmented generation, comparing and contrasting parametric knowledge with contextual information, and how to enhance knowledge on large language models.

This section summarizes key articles that informed this research.

\subsection{Foundational Papers on Large Language Models}

Large language models have exploded in popularity since the development of transformer architecture \citep{attention_is_all_you_need}.
This architecture relies entirely on self-attention mechanisms rather than recurrent layers, which allow the model to weigh the importance of different words in a sequence relative to each other, irrespective of their position.
This mechanism enables the model to capture complex dependencies and relationships across long sequences more effectively than traditional models.

\begin{figure}[htb]
	\centering
	\includegraphics[width=.37\textwidth]{transformers.png}
	\caption{Transformer Architecture, from ``Attention is all you need'' \citep{attention_is_all_you_need}.}
\end{figure}

GPT models \citep{gpt} improve upon this architecture by running a supervised task-specific fine-tuning round after the unsupervised pre-training on large amount of test data.
Later models, starting from GPT-2, use zero-shot transfer learning to improve their performance \citep{gpt2}.
Zero-shot learning \citep{zeroshotlearning} trains a model to perform a task without having been explicitly trained on examples of that task; instead, it leverages knowledge gained from pre-training to infer and generalise to new tasks with the context on the prompt.

By adding only a few examples of the task at hand, a model can improve generalisation to new tasks with a limited amount of labelled data.
GPT-3 uses this to understand the structure and nature of a task with a few examples \citep{gpt3}.

Despite these improvements in sequential models, the models still present many faults in question-and-answer tasks \citep{how_can_we_know}.
Hallucinations are not uncommon, even for queries that ask questions that can be answered with short answers.

\subsection{Architectures of Large Language Models}
\label{llm_architectures}

In this subsection we present two different large language model architectures that are used widely for this research.

\subsubsection{T5 and Flan-T5}

The T5 model \citep{t5}, developed by Google Brain in 2020, is an encoder-decoder Seq2Seq model that treats every processing problem as a text-to-text task.
This way it uses transfer learning to learn many different kinds of tasks from a single source of training data.

Besides using a transformers architecture \citep{attention_is_all_you_need}, one of the novel \TODO{complete}.

\subsection{Retrieval-Augmented Generation}

Large pre-trained language models store factual knowledge in their parameters.
Their ability to access factual information from their source data without the risk of hallucinating incorrect information is limited, which affects their performance on knowledge-intensive tasks such as question-and-answer problems.

Retrieval-Augmented Generation (RAG) attempts to solve this problem by adding extra non-parametric data in a context gathered from an index with a separately-trained retriever \citep{rag}.
This retrieved context is fed back to the original query as a combined representation containing this extra data.
A diagram with an overview of this method is presented in \cref{rag_overview}.

\begin{figure}[htp]
	\centering
	\includegraphics[width=.9\textwidth]{rag-example.png}
	\caption{An overview of the RAG approach, combining a pre-trained retriever with a pre-trained model. From ``Retrieval-Augmented Generation for Knowledge-Intensive NLP tasks'' \citep{rag}}
	\label{rag_overview}
\end{figure}

RAG can be effective in preventing hallucinations by incorporating relevant external information into the generation process, ensuring that responses are more grounded in factual data from a knowledge base.
However, this method is not perfect even after adding contextual data to the query the model could generate a response using its learned parametric memory while ignoring the data retriever by the RAG model.

\subsection{Knowledge Grounding on Queries with Added Context}

Knowledge grounding is the process by which a large language model incorporates external knowledge into its output generation.
In the context of queries with context provided by RAG, a well-grounded model would ensue that answers are consistent with the knowledge in the index rather than in the model's inherent knowledge if they are contradictory.

Various attempts have been made at understanding how good the knowledge grounding of a RAG system is, and on what is the optimal configuration of RAG.

In ``RAGGED: Towards Informed Design of Retrieval Augmented Systems'' \citep{ragged}, the authors create a framework to evaluate these systems and find that different models suit substantially varied RAG setups.
In particular, the performance of the models decreases strongly in Decoder-only models such as Llama when the context passages provides more context, while Seq2Seq models such as Flan-T5 have better performance.

``Characterizing Mechanisms for Factual Recall in Language Models'' \citep{factual_recall}, which is one of the main sources for this thesis, finds that when there is disagreement between the model's knowledge and the provided context then the architecture and size of the large language model will affect the probability of the model choosing to use the context (which is much less prone to hallucinating) as an answer rather than its parametric knowledge.

This paper also introduces a novel way to test this hypothesis by creating a dataset with questions and counterfactual context, an example of which is shown in \cref{query_example}.
By adding counterparametric information to the context, this method allows us to understand whether an answer came is parametric (that is, came from the memory of the model) or contextual (that is, came from the provided context).

\begin{lstlisting}[caption={Example of queries used in \citep{factual_recall}. These queries form the basis and inspiration for the dataset creation done in this thesis}, label={query_example},basicstyle=\ttfamily\small,keywordstyle=\rmfamily\bfseries,keywords={country,in,context,city},captionpos=b,frame=single,breaklines=true,xrightmargin=.15\textwidth,xleftmargin=.15\textwidth,float=h]
The capital of {country} is {in context city}
Q: What is the capital of {country}?
A:
\end{lstlisting}
