\section{Related Work}

This research is the latest on a long line of academic articles on the topics of retrieval-augmented generation, counterparametric and contextual data, and how to enhance knowledge on large language models.

This section presents a short summary of some of the articles that were useful in researching this topic.

\footnotetext[1]{This entire section is in progress --- short summaries of the named papers will come soon.}

\subsection{Foundational Papers on Large Language Models}
\newcommand{\mart}[1]{\begin{itemize} \item #1 \end{itemize}}
\begin{itemize}
	\item ``Language models are unsupervised multitask learners''\citep{gpt2}.
		\mart{The foundational paper for GPT2.}
	\item ``Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer'' \citep{t5}.
		\mart{The foundational paper for T5.}
	\item ``Language Models are Few-shot Learners'' \citep{fewshotlearners}.
		\mart{Introduces ``in-context learning''.}
	\item ``Prompt programming for large language models: Beyond the few-shot paradigm'' \citep{beyondfewshot}.
		\mart{Improves the previous paper.}
\end{itemize}

\subsection{Papers working with RAG and contextual data}
\begin{itemize}
	\item ``Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'' \citep{rag}.
		\mart{Foundational paper for RAG.}
	\item ``Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection'' \citep{selfrag}.
		\mart{Interesting RAG system.}
	\item ``Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model'' \citep{can_rag_models_reason}.
		\mart{Nice evaluation of RAG models.}
\end{itemize}

\subsection{Retrieval-Augmented Language Models}
\begin{itemize}
	\item ``Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study'' \citep{shall_we_pretrain_autoregressive}.
		\mart{Reproduces and pretraines RETRO.}
	\item ``Atlas: Few-shot Learning with Retrieval Augmented Language Models'' \citep{atlas_foundational}.
		\mart{Introduces ATLAS.}
	\item ``Improving language models by retrieving from trillions of tokens'' \citep{retro}.
	\item ``RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems'' \citep{ragged}.
		\mart{Analyses results of these systems; compares Llama to Flan-T5.}
\end{itemize}

\subsection{On disentangling parametric and context-augmented counterparametric knowledge}
\begin{itemize}
	\item ``DISCO: Distilling Counterfactuals with Large Language Models'' \citep{disco}.
		\mart{Does  similar analysis with counterfactuals to this thesis}.
	\item ``DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering'' \citep{disentqa}.
		\mart{Also does a similar analysis to this thesis.}
	\item ``Characterizing Mechanisms for Factual Recall in Language Models'' \citep{factual_recall}.
		\mart{Very simple analysis, but tries to understand \textsc{where} in the model the contextual answers come from.}
	\item ``Can We Edit Factual Knowledge by In-Context Learning?'' \citep{can_we_edit_factual_knowledge}.
	\item ``Learning the Difference that Makes a Difference with Counterfactually-Augmented Data'' \citep{learning_the_difference}.
\end{itemize}

