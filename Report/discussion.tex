\section{Discussion}
\label{discussion}

\Cref{results_section} presented several results from generating the question dataset and running the framework to understand the role of parametric knowledge in question answering with a context.

This section attempts to explain these results, and discusses what they mean for our research question.

\subsection{Model architecture and memorised knowledge}
\label{model_architecture_parametric}

\Cref{framework_results} presents the results of running our framework on the provided questions on two different model architectures: Seq2Seq models and Decoder-only models.
\Cref{flan_cats_table,llama_cats_table} show these results split by category of question.

The results are clear: \textbf{Seq2Seq models tend to answer questions from their contextual knowledge rather than from their inherent knowledge more often than Decoder-only models}.
These results are the same regardless of the category of the question, and are consistent among many types and lengths of answers.

This results shows that, in the framework of question-answering when using RAG to fetch contextual data from an index, Seq2Seq models will have fewer hallucinations that contradict this index than Decoder-only models.

We present several hypotheses which might cause the difference in these results.

\subsubsection{Encoder-Decoder Architecture}

As described in \cref{llm_architectures}, Seq2Seq models such as \texttt{Flan-T5} are encoder-decoder models that process the entire context of the query in the encoder component before passing it to the decoder, which could increase the weight given to the context itself.

We can test this hypothesis by finding the average attention of the context section of each query and of the question section for each model, using the method discussed in \cref{attention_section}.

\subsection{Model size and memorised knowledge}
\label{model_size_parametric}

\Cref{framework_results} also shows differences in how models of different sizes process information in queries with counterparametric context.

\subsubsection{Seq2Seq Models}

While the average results are very similar, which is likely due to the properties of Seq2Seq models discussed in \cref{model_architecture_parametric}, the models seem to be significantly different for queries of category \texttt{element}, \texttt{historical\_events}, and a few others.
\Cref{seq2seq_difference} presents some cases where the results are different.

\subsection{What are all of these Others?}

\Cref{parametric_vs_contextual} showed that a significant minority of responses to queries with counterfactual context are \Other{}: answers that aren't equal to either the parametric nor the contextual data.
The numbers of these entries, per model, are presented in \cref{others_list}.

\begin{table}[h]
	\centering
	\footnotesize
	\begin{tabular}{>{\bfseries}l | c c c c}
		\toprule
			& \ttfamily\scriptsize Flan-T5-XL & \ttfamily\scriptsize Flan-T5-XXL & \ttfamily\scriptsize \llamaparbox{} & \ttfamily\scriptsize \bigllamaparbox{} \\
		\midrule
			\Other{} & 260 & 192 & 312 & 387 \\
		\bottomrule
	\end{tabular}
	\caption{Amount of \Other{} entries, that is, results where the answer was not either the parametric or contextual answer.}
	\label{others_list}
\end{table}

By manually checking these results, we can understand the reason why the model chose these answers.

\begin{description}[style=nextline]
	\item[1. Different phrasing of a parametric answer]
		There are many answers where the parametric chooses a certain way to phrase some answer, while the contextual information biases it to give the parametric answer with a different context.
	\item[2. Plain incorrect answers]
		Sometimes, adding counterfactual context to the model just causes it to produce an incorrect answer that's different the answers given by both the model and the context.
	\item[3. Question misinterpretation due to the context]
		Some questions can be ambiguous or have a low probability of another answer.
		By adding a context with a counterfactual answer, the model can misinterpret the question and answer something different.
	\item[4. Negating the context]
		This is an interesting one: if the model has an answer in its parametric knowledge that contradicts the data on its context, then it interpret the context as part of the question and adds its negation as part of the answer.
	\item[5. Different phrasing of the context]
		Much less common than point 1, models sometimes give the same answer as the context but in the format of the parametric answer.
	\item[6. Correct answer, just different than the parametric answer]
		Some question have multiple correct answers, and adding counterfactual context can cause the model to give a different one.
	\item[7. Mixing elements of both parametric answer and context]
		Elements of the parametric answer are mixed with elements of the context in the model's answer.
		This produces an incorrect answer, but it's easy to understand where it came from.
		The cause of this is likely the greedy decoding used to find the answers, as explained in \cref{methodology_type_of_answer}.
\end{description}

Examples of each one of these types can be found in \cref{other_examples}.

Does the architecture and size of the model affect the distribution of each type of \Other{} answer?
\Cref{other_results_category} contains the amount of answers per architecture for each one of the models, and there does not seem to be a particular distribution.

\begin{table}[ht]
	\centering
	\footnotesize
	\begin{tabular}{>{\bfseries}r | r r r r}
		\toprule
			\bfseries Type & \ttfamily\scriptsize Flan-T5-XL & \ttfamily\scriptsize Flan-T5-XXL & \ttfamily\scriptsize \llamaparbox{} & \ttfamily\scriptsize \bigllamaparbox{} \\
		\midrule
			\tiny (\Parametric{}) & 248 & 242 & 745 & 1070 \\
			\tiny (\Contextual{}) & 4284 & 4304 & 3662 & 3303 \\
		\midrule
			1. & 0 & 0 & 116 & 234 \\
			2. & 6 & 3 & 50 & 15 \\
			3. & 0 & 0 & 13 & 8 \\
			4. & 0 & 0 & 20 & 61 \\
			5. & 241 & 170 & 33 & 38 \\
			6. & 7 & 16 & 63 & 23 \\
			7. & 6 & 3 & 17 & 8 \\
		\bottomrule
	\end{tabular}
	\caption{Different types of \Other{} answers per model (with amount of \Parametric{} and \Contextual{} added for comparison). The two most notable groups are \textbf{1.}, which contains parametric answers with different phrasing, and \textbf{5.}, which contains counterfactual answers with different phrasing.}
	\label{other_results_category}
\end{table}

Surprisingly, there is a large difference in the distribution of answers that don't come either from the model or from the given context.

In the case of Seq2Seq models, the vast majority of \Other{} answers are \Contextual{} answers which have a different format due to model mangling.
This is consistent with the previous result, where the vast majority of their answers came from the query's context.

The majority of \Other{} answers in Decoder-only models are the opposite: \Parametric{} answers that keep the format of the context.
However, the reasons are much more varied and this would be an interesting topic of discussion in future research.

Part of the reason for many of these answers, particularly on the Seq2Seq models, is due to the strict comparison function we use to define answer type.
This is an area that should be improved; \cref{other_problems} gives more information and various suggestions on how to compare results that might be relevant on future work.

\begin{table}[p]
	\centering
	\scriptsize
	\renewcommand{\arraystretch}{1.5}
	\begin{threeparttable}
		\begin{tabularx}{\textwidth}{>{\bfseries}c>{\ttfamily}X >{\ttfamily}p{75pt} >{\ttfamily}p{75pt} >{\ttfamily}p{75pt}}
			\toprule
			\bfseries Reason & \rmfamily\bfseries Question & \rmfamily\bfseries Parametric & \rmfamily\bfseries Counterfactual & \rmfamily\bfseries Contextual \\
			\midrule
			\multirow[t]{2}{*}{1.} & Who was the primary leader associated with The Reforms of Diocletian? & Diocletian Himself & Caracalla, a Roman Emperor & Diocletian, a Roman Emperor \\
			& In which city is Louvre Pyramid located? & Paris, France & the city of Valladolid, in the state of Yucatan, Mexico & the city of Paris, in the country of France \\
			\multirow[t]{1}{*}{2.} & In which period of the periodic table is Silver located? & 5 of the periodic table & 3 of the periodic table & 4 of the periodic table \\
			\multirow[t]{2}{*}{3.} & What was the duration of Queen Elizabeth II's Platinum Jubilee?	& 12 months & approximately 100 years & approximately 70 years \\
			& What is the nearest major body of water to Cusco? & Lake Titicaca & the North Sea & the Pacific Ocean \\
			\multirow[t]{2}{*}{4.} & What is the name of the main protagonist in One Flew Over the Cuckoo's Nest? & Randle McMurphy & Achilles & Not Achilles \\
			& What is Frida Kahlo primarily known for? & her self-portraits and her depiction of Mexican culture & his theories on communism and his critique of capitalism & her artwork and her life story, not for his theories on communism or critique of capitalism \\
			\multirow[t]{1}{*}{5.} & How many pages are in One Flew Over the Cuckoo's Nest? & 320 pages	& 480 pages in a standard edition & 480 pages \\
			\multirow[t]{3}{*}{6.} & Who is credited with the discovery of Conservation of Energy? & Julius Robert Mayer & Alfred Wegener & James Joule\tnote{1} \\
			& What is the name of the main protagonist in The Great Gatsby? & Nick Carraway	& Liesel Meminger & Jay Gatsby\tnote{2} \\
			& What educational institution did Srinivasa Ramanujan attend? & The University of Madras & The University of Vienna & the University of Cambridge\tnote{3} \\
			\multirow[t]{2}{*}{7.} & What is the date of death of Vladimir Lenin? & January 21, 1924 & March 28, 1941 & March 21, 1924 \\
				& What's the main nationality of Mozart? & Austria & English-Born American & American-born English-born Austrian \\
			\bottomrule
		\end{tabularx}
		\begin{tablenotes}
\item[1] Discovery of the conservation of energy is credited to both Julius Robert Mayer and James Joule.
\item[2] Nick Carraway and Jay Gatsby are co-protagonists in The Great Gatsby.
\item[3] Srinivasa Ramanujan attended both the University of Madras and the University of Cambridge.
		\end{tablenotes}
	\end{threeparttable}
	\caption{Examples of different types of \Other{} answers when running the \texttt{Meta-Llama-3.1-8B-Instruct} model. Other models have similar reasons for these kinds of answers.}
	\label{other_examples}
\end{table}

\subsection{Differences in perplexity scores for larger and smaller models}

\subsubsection{Can we use this to predict from where an answer came from?}

\subsection{Differences in distributions for different categories and questions.}
