\section{Methods}

\subsection{Creating a representative dataset of questions}
\label{creating_dataset}

As argued in \Cref{questions_objective}, our codebase requires the creation of a new dataset of questions with three main properties.

\begin{enumerate}
	\item The questions should have short an unambiguous answers. \label{q_short}
	\item They must cover a large set of topics, eras, and places. \label{q_widespread}
	\item They must allow for the creation of sensible counterparametric answers (different than what the model would normally answer) by having sets of questions with the same answer format. \label{q_counterfactual}
\end{enumerate}

To address these items, we follow the approach done by \citeauthor{factual_recall} in creating base questions that refer to a specific object, so all the answers for the same base question have a similar format and creating counterparametric answers is easy.

Since this thesis requires a set of questions that covers a large set of topics, eras, and places, we enhance this method by creating a set of categories, each of which has a large set of base questions and another set of objects that can be matched.
An example of this approach is shown in \cref{source_data_example}.

\begin{table}[h]
	\setlength{\fboxsep}{0pt}
	\setlength{\fboxrule}{1pt}
	\newcommand{\rep}[1]{{\setlength{\fboxsep}{0pt}\fcolorbox{Gray}{Gray!80}{\textit{#1}}}}

	\centering
	\scriptsize
	\begin{tabular}{>{\bfseries}c | l | c | l}
		\toprule
			\bfseries Category & \bfseries Base Questions & \bfseries Object & \bfseries Queries \\
		\midrule
			Person & \begin{minipage}{.30\textwidth}
				\ttfamily
				Q: What is the date of birth of \rep{\{person\}}? \\ A: The date of birth of \rep{\{person\}} is \\[1ex]
				Q: In what city was \rep{\{person\}} born? \\ A: \rep{\{person\}} was born in
			\end{minipage} &
			\begin{minipage}{.12\textwidth}
				\ttfamily
				\textcolor{Red}{Che~Guevara} \\[1ex]
				\textcolor{Sepia}{Confucius}
			\end{minipage} &
			\begin{minipage}{.40\textwidth}
				\ttfamily
				Q: What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ A: The date of birth of \textcolor{Red}{Che~Guevara} is \\[1ex]
				Q: What is the date of birth of \textcolor{Sepia}{Confucius}? \\ A: The date of birth of \textcolor{Sepia}{Confucius} is \\[1ex]
				Q: In what city was \textcolor{Red}{Che~Guevara} born? \\ A: \textcolor{Red}{Che~Guevara} was born in \\[1ex]
				Q: In what city was \textcolor{Sepia}{Confucius} born? \\ A: \textcolor{Sepia}{Confucius} was born in
			\end{minipage} \\
		\midrule
			City & \begin{minipage}{.30\textwidth}
				\ttfamily
				Q: What country is \rep{\{city\}} in? \\ A: \rep{\{city\}} is in
			\end{minipage} &
			\begin{minipage}{.10\textwidth}
				\ttfamily
				\textcolor{BurntOrange}{Cairo} \\[1ex]
				\textcolor{ForestGreen}{Mumbai} \\[1ex]
				\textcolor{Cyan}{Buenos Aires} \\[1ex]
				\textcolor{Purple}{London}
			\end{minipage} &
			\begin{minipage}{.40\textwidth}
				\ttfamily
				Q: What country is \textcolor{BurntOrange}{Cairo} in? \\ A: \textcolor{BurntOrange}{Cairo} is in \\[1ex]
				Q: What country is \textcolor{ForestGreen}{Mumbai} in? \\ A: \textcolor{ForestGreen}{Mumbai} is in \\[1ex]
				Q: What country is \textcolor{Cyan}{Buenos Aires} in? \\ A: \textcolor{Cyan}{Buenos Aires} is in \\[1ex]
				Q: What country is \textcolor{Purple}{London} in? \\ A: \textcolor{Purple}{London} is in
			\end{minipage} \\
		\bottomrule
	\end{tabular}
	\caption{Some examples of the base-question and object generation that are fed to the models for finding parametric answers.}
	\label{source_data_example}
\end{table}

This list of questions will enable the research on whether the answers given by large language models depend on the category and the format of the questions.

\subsection{When does a model choose the provided context knowledge over its inherent knowledge?}
\label{method22}

\subsubsection{Model Selection}

In order to get a general understanding of large language models with added context, we test the queries generated in \cref{creating_dataset} into four models of different types and sizes.

\begin{table}[h]
	\centering
	\begin{tabular}{>{\bfseries}c@{\hspace{20pt}}l l}
		\toprule
			& \bfseries Seq2Seq Model & \bfseries Decoder-Only Model \\
		\midrule
			Small & \ttfamily Flan-T5-XL & \ttfamily Meta-Llama-3.1-8B-Instruct \\
			Large & \ttfamily Flan-T5-XXL & \ttfamily Meta-Llama-3.1-70B-Instruct \\
		\bottomrule
	\end{tabular}
	\caption{The four large language models chosen for this research.}
\end{table}

The Flan-T5 models \citep{flant5} are an extension to the original Seq2Seq T5 models \citep{t5} which are fine-tuned to particular NLP tasks framed as text-to-text problems.
Compared to T5, it's generally better at following instructions and has improved zero-shot performance.

The Llama models \citep{llama3} are Decoder-only models with a dense transformer architecture that are fine-tuned for instruction-following tasks, and are specially adept at complex prompts.

\subsubsection{What type of answer does each model select for each question?}

The first step to understanding the knowledge grounding of large language models is to create queries that contain counterparametric data as part of the context.
By comparing the result to the existing answers it becomes trivial to understand whether an answer came from the model's memory, the queries' context, or neither of these.

Following the approach of \citeauthor{factual_recall}, for every query we randomly sample from the set of answers of the same base question for answers that are different to the parametric answer (given by the original query).
Later, we add this \emph{counterparametric answer} to the context, to form a new query and query the same model again.

\begin{figure}[ht]
	\centering
	\fbox{\includegraphics[width=.75\textwidth]{Method.png}}
	\caption{Example diagram of steps used to calculate the two sets of answers, \textit{parametric} and \textit{contextual}, and to compare them to answer the question in this objective. Many of the terms in this diagram are explained in the \protect\hyperref[glossary]{Glossary}.}
	\label{action_diagram}
\end{figure}

To ensure that the results are simple to interpret and minimise the effect of randomness, once we select the queries we follow the example of \citeauthor{ragged} and use Greedy Decoding to find the answer.

% While beam search with a short beam width tends to produce more accurate results for long answers \citep{sutskever_seq2seqlearning,wu_mltranslation} and there are many other sampling methods that produce better results \citep{text_degeneration}, this is likely to not have an effect on experiments that result in shorter answers \citep{t5}.

We compare the parametric answer to the previous values to come to one of three cases: either this answer is identical to the \textbf{Parametric} answer and the model inferred it from its memor, to the \textbf{Contextual} answer and the model inferred it from the context, or the answer is different to these two and the model inferred it from some \textbf{Other} place.

This approach is detailed in \cref{action_diagram}; \cref{counterparametric_table} contains an example of the shuffling done for this experiment while \cref{category_example} contains an example of each of the three categories.

\begin{table}[htbp]
	\newcommand{\vwidth}[1]{\begin{minipage}[t][][t]{38ex}\ttfamily #1\end{minipage}}
	\newcommand{\rep}[1]{{\setlength{\fboxsep}{0pt}\fcolorbox{Gray}{Gray!80}{\textit{#1}}}}

	\centering
	\scriptsize

	\begin{tabularx}{\textwidth}{>{\ttfamily}l@{\hspace{0pt}}|>{\ttfamily}c@{\hspace{1pt}}>{\ttfamily}c@{\hspace{0pt}}>{\ttfamily}c@{\hspace{22pt}}>{\ttfamily}X}
		\toprule
			\rmfamily \bfseries Base Question & \rmfamily \bfseries Object & \rmfamily \bfseries \parbox{40pt}{\centering Parametric Answer} & \rmfamily \bfseries \parbox{75pt}{\centering Counterparametric Answer} & \rmfamily \bfseries \parbox{100pt}{Question with counterparametric context} \\
		\midrule
			\multirow{4}{65pt}[-100pt]{Q: What is the date of birth of \protect\rep{\{person\}}? \\ A: The date of birth of \protect\rep{\{person\}} is} & \textcolor{Red}{Che~Guevara} &  \textcolor{Red}{June~14,~1928} & \textcolor{Apricot}{965~AD} & \vwidth{Context: [the date of birth of \textcolor{Red}{Che~Guevara} is \textcolor{Apricot}{965~AD}]. \\ Q: What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ A: The date of birth of \textcolor{Red}{Che~Guevara} is} \vspace{4pt} \\
			& \textcolor{Apricot}{Ibn~al-Haytham} &  \textcolor{Apricot}{965~AD} & \textcolor{Red}{June~14,~1928} & \vwidth{Context: [the date of birth of \textcolor{Apricot}{Ibn~al-Haytham} is \textcolor{Red}{June~14,~1928}]. \\ Q: What is the date of birth of \textcolor{Apricot}{Ibn~al-Haytham}? \\ A: The date of birth of \textcolor{Apricot}{Ibn~al-Haytham} is} \vspace{4pt} \\
			& \textcolor{Blue}{Boyan~Slat} &  \textcolor{Blue}{27~January~1994} & \textcolor{Brown}{February~23,~1868} & \vwidth{Context: [the date of birth of \textcolor{Blue}{Boyan~Slat} is \textcolor{Brown}{February~23,~1868}]. \\ Q: What is the date of birth of \textcolor{Blue}{Boyan~Slat}? \\ A: The date of birth of \textcolor{Blue}{Boyan~Slat} is} \vspace{4pt} \\
			& \textcolor{Brown}{W.E.B~Du~Bois} &  \textcolor{Brown}{February~23,~1868} & \textcolor{Red}{June~14,~1928} & \vwidth{Context: [the date of birth of \textcolor{Brown}{W.E.B~Du~Bois} is \textcolor{Red}{June~14,~1928}]. \\ Q: What is the date of birth of \textcolor{Brown}{W.E.B~Du~Bois}? \\ A: The date of birth of \textcolor{Brown}{W.E.B~Du~Bois} is} \vspace{4pt} \\
		\midrule
			\multirow{2}{65pt}[-25pt]{Q: What country is \protect\rep{\{city\}} in? \\ A: \protect\rep{\{city\}} is in}
			& \textcolor{BurntOrange}{Cairo} &  \textcolor{BurntOrange}{Egypt} & \textcolor{ForestGreen}{India} & \vwidth{Context: [\textcolor{BurntOrange}{Cairo} is in \textcolor{ForestGreen}{India}]. \\ Q: What country is \textcolor{BurntOrange}{Cairo} in? \\ A: \textcolor{BurntOrange}{Cairo} is in} \vspace{4pt} \\
			& \textcolor{ForestGreen}{Mumbai} &  \textcolor{ForestGreen}{India} & \textcolor{BurntOrange}{Egypt} & \vwidth{Context: [\textcolor{ForestGreen}{Mumbai} is in \textcolor{BurntOrange}{Egypt}]. \\ Q: What country is \textcolor{ForestGreen}{Mumbai} in? \\ A: \textcolor{ForestGreen}{Mumbai} is in} \vspace{4pt} \\
		\bottomrule
	\end{tabularx}
	\caption{Using the same question format allows us to repurpose previous parametric answers as counterparametric ones.}
	\label{counterparametric_table}
\end{table}

\begin{table}[htbp]
	\centering
	\scriptsize

	\begin{tabularx}{\textwidth}{>{\ttfamily}X >{\ttfamily}l c}
		\toprule
			\bfseries \textrm Query & \bfseries \textrm Answer & \bfseries Category \\
		\midrule
			\parbox{235pt}{Context: [the nearest major body of water to \textcolor{Dandelion}{Windhoek} is the \textcolor{RoyalPurple}{Rio de la Plata}] \\ Q: What is the nearest major body of water to \textcolor{Dandelion}{Windhoek}? \\ A: The nearest major body of water to \textcolor{Dandelion}{Windhoek} is} &
			\textcolor{Dandelion}{the Atlantic Ocean} & 
			\bfseries \textcolor{ForestGreen}{Parametric} \\[8ex]
			%
			\parbox{235pt}{Context: [the date of birth of \textcolor{Red}{Che~Guevara} is \textcolor{Apricot}{965~AD}]. \\ Q: What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ A: The date of birth of \textcolor{Red}{Che~Guevara} is} &
			\textcolor{Apricot}{965~AD} &
			\bfseries \textcolor{Maroon}{Contextual} \\[8ex]
			%
			\parbox{235pt}{Context: [\textcolor{Purple}{Rome} is in \textcolor{Salmon}{Georgia}] \\ Q: What country is \textcolor{Purple}{Rome} in? \\ A: \textcolor{Purple}{Rome} is in} &
			\textcolor{BlueViolet}{the United States} &
			\bfseries \textcolor{MidnightBlue}{Other} \\[4ex]
		\bottomrule
	\end{tabularx}
	\caption{Example for results with \textbf{Parametric}, \textbf{Contextual}, and \textbf{Other} values. Note that, in the third query, the model is interpreting the question as referring to Rome in the US State of Georgia, rather than the country of Georgia.}
	\label{category_example}
\end{table}

\subsection{Can we use the perplexity score of an answer to predict whether it came from inherent or contextual knowledge?}
\label{method_perplexity}

\subsubsection{Perplexity Score}
\newcommand{\NLL}{\text{NLL}}
\newcommand{\PPL}{\text{PPL}}

The Perplexity score of an answer is normally used to measure the inverse of the certainty that the model has of a particular answer \citep{fewshotlearners,retro}.
In a sense, it's the ``surprise'' of a model that a certain answer is correct.

We can define the probability of a model choosing a token $x_n$ with context $x_1, \dots, x_{n - 1}$ from a query $Q$ by calculating the softmax value of all the logits for the possible words for this token.

The probabilities of the tokens if an answer can be accumulated to calculate the negative log-likelihood $\NLL$, which is used to calculate the perplexity $\PPL$ using the formulas from \cref{eq:nll,eq:ppl}.

\begin{align}
	\NLL \left( x_1, \dots, x_n \mid Q \right) &= - \frac{1}{n} \sum^n_{i = 1} \log_2 P \left( x_i \mid Q, x_1, \dots, x_{i - 1} \right) \label{eq:nll} \\
	\PPL \left( x_1, \dots, x_n \mid Q \right) &= {2 ^ {\text{NLL} \left( x_1, \dots, x_n \mid Q \right)}} \label{eq:ppl}
\end{align}

\newpage{}

\subsubsection{Perplexity of the parametric answer with counterparametric context and vice-versa}

Note that the token $x_n$ does not necessarily have to be the result of applying the query $x_1, \dots, x_{n - 1}$ to a model.

Therefore, it becomes necessary to use teacher-forcing \citep{teacher_forcing} to feed some answer to the model regardless of what's the answer to this particular query. This allows us to calculate the perplexity scores of the parametric answers for both the regular query and the one with counterparametric context, and the perplexity scores of the contextual answers for these two queries.

For a given parametric answer $p_1, \dots, p_n$ and randomly sampled counterparametric answer $q_1, \dots, q_m$, a query without context $Q$, and a query with this counterparametric context $Q'$ we can calculate four different perplexity scores as shown in \cref{perplexity_table}.

\begin{table}[h]
	\footnotesize
	\newcommand{\vmini}[1]{$#1$}

	\centering
	\renewcommand{\arraystretch}{3}
		\begin{tabular}{  l >{\centering}p{.2\textwidth} | >{\centering}p{.3\textwidth} | p{.3\textwidth} | }
		\cline{3-4}
			& & \multicolumn{2}{c|}{\raisebox{11pt}{\bfseries Tokens}} \\[-15pt]
		\cline{3-4}
			& & \raisebox{11pt}{Parametric $p$} & \raisebox{11pt}{\hspace{20pt} Counterparametric $q$} \\[-15pt]
		\hline
			\multicolumn{1}{ | c | }{\multirow[b]{2}{*}{\rotatebox{90}{\bfseries \centering  Context}}}
			& Regular Query &
			$P_0 = \PPL \left( p_1, \dots, p_n \mid Q \right)$ &
			$P_1 = \PPL \left( q_1, \dots, q_{m} \mid Q \right)$ \\
		\cline{2-4}
			\multicolumn{1}{ | c | }{} & Using counterparametric context &
			$P_2 = \PPL \left( p_1, \dots, p_n \mid Q' \right)$ &
			$P_3 = \PPL \left( q_1, \dots, q_{m} \mid Q' \right)$ \\
		\hline
	\end{tabular}
	\caption{Four different perplexity values: one for each set of tokens, and one for each query context.}
	\label{perplexity_table}
\end{table}

Since the parametric answer is by definition the response of the model to the regular query, $P_0 \leq P_1$.
In fact, the perplexity of the parametric value is lower than the perplexity of any other answer on query $Q$.

The question in \cref{method22} is equivalent to asking whether $P_2 \lesseqqgtr P_3$, or whether there is a different sequence of tokens that has any lower perplexity of these two.

\subsubsection{Predicting whether an answer came from memory or from context}

One question remains: if the response of the query with counterparametric context $Q'$ is $x_1, \dots, x_n$, how can we predict whether this answer is came from the model's memory $p$, from the given context $q$, or something else?

We propose investigating the value of the perplexity $\PPL \left( x_1, \dots, x_n \mid Q' \right)$ and comparing it to the distribution of perplexities on parametric and contextual answers. TODO: Maybe include a KDE or a K-S test here.
