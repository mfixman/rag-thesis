\section{Methodology}

\newcommand{\cats}{7}
\newcommand{\baseqs}{68}
\newcommand{\things}{312}
\newcommand{\qs}{3840}

\subsection{Source Data Preparation}

Our source data is prepared by extending the ideas presented by Yu et al\cite{factual_recall}.
Instead of using one simple question, our approach consists of separating this data into \cats{} categories, where each category has a set of base questions and another set of objects that are paired together and presented to our models.

This work contains \cats{} categories in the configuration shown by \cref{categories_numbers}, for a total of \qs{} questions.
The full list of questions can be found in \cref{appendixA}.

\begin{table}[h]
	\centering
	\scriptsize
	\begin{tabular}{l | r r r}
		\toprule
			\bfseries Category & \bfseries Questions & \bfseries Objects & \bfseries Total \\
		\midrule
			Person           & 14 &  47 &  658 \\
			City             & 14 &  60 &  840 \\
			Principle        & 10 &  30 &  300 \\
			Element          & 10 &  35 &  350 \\
			Book             & 10 &  45 &  450 \\
			Painting         & 14 &  39 &  546 \\
			Historical Event & 6  &  56 &  336 \\
		\midrule
			Total & \baseqs{} & \things{} & \qs{} \\
		\bottomrule
	\end{tabular}
	\caption{The amount of questions for each category. The full list of questions can be found in \cref{appendixA}. This is still a work in progress and I expect to add more questions.}
	\label{categories_numbers}
\end{table}

We enhance the zero-shot learning prompt used by Brown~et.~al\cite{fewshotlearners} by using the prompt format example format presented by Jiang~et.~al\cite{how_can_we_know} for calibrating the T5 language model by adding both the question and the first part of the answer.
Examples of this preparation can be found in \cref{source_data_example}.

\begin{table}
	\setlength{\fboxsep}{0pt}
	\setlength{\fboxrule}{1pt}
	\newcommand{\rep}[1]{\fcolorbox{Gray}{Gray!80}{\textit{#1}}}

	\centering
	\scriptsize
	\begin{tabular}{l | c | l}
		\toprule
			\bfseries Base Question & \bfseries Object & \bfseries Final Question \\
		\midrule
			\begin{minipage}{.39\textwidth}
				\ttfamily
				What is the date of birth of \rep{\{person\}}? \\ The date of birth of \rep{\{person\}} is \\[1ex]
				In what city was \rep{\{person\}} born? \\ \rep{\{person\}} was born in \\[1ex]
				What country is \rep{\{city\}} in? \\ \rep{\{city\}} is in
			\end{minipage} &
			\begin{minipage}{.16\textwidth}
				\centering
				\ttfamily
				\fcolorbox{Gray!50}{Gray!50}{\textcolor{Red}{Che~Guevara}} \\[1ex]
				\fcolorbox{Gray!50}{Gray!50}{\textcolor{Sepia}{Confucius}} \\[1ex]
				\fcolorbox{Gray!50}{Gray!50}{\textcolor{BurntOrange}{Cairo}} \\[1ex]
				\fcolorbox{Gray!50}{Gray!50}{\textcolor{ForestGreen}{Mumbai}}
			\end{minipage} &
			\begin{minipage}{.45\textwidth}
				\ttfamily
				What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ The date of birth of \textcolor{Red}{Che~Guevara} is \\[1ex]
				What is the date of birth of \textcolor{Sepia}{Confucius}? \\ The date of birth of \textcolor{Sepia}{Confucius} is \\[1ex]
				In what city was \textcolor{Red}{Che~Guevara} born? \\ \textcolor{Red}{Che~Guevara} was born in \\[1ex]
				In what city was \textcolor{Sepia}{Confucius} born? \\ \textcolor{Sepia}{Confucius} was born in \\[1ex]
				What country is \textcolor{BurntOrange}{Cairo} in? \\ \textcolor{BurntOrange}{Cairo} is in \\[1ex]
				What country is \textcolor{ForestGreen}{Mumbai} in? \\ \textcolor{ForestGreen}{Mumbai} is in
			\end{minipage} \\
		\bottomrule
	\end{tabular}
	\caption{Some examples of the base-question and object generation that are fed to the models for finding parametric answers.}
	\label{source_data_example}
\end{table}

\subsection{Prompting}

Previous research on this area uses a very simple query format without any prompt before the question\cite{how_can_we_know}\cite{factual_recall}.
While this technique is sufficient when solely looking for answers, its answers cause problems when calculating the likelihood and perplexity of the answers.

To prevent language models from adding context in the problem of zero-shot learning, it's necessary to program a custom prompt\cite{beyondfewshot}.
Due to the dual-prompting nature of this problem for finding parametric and counterparametric knowledge, two different prompts are defined as defined in \cref{prompts}.

\begin{figure}[h]
	\fbox{
		\begin{minipage}[c][50pt]{.45\textwidth}
			\ttfamily
			Answer the following question in a few words and with no formatting.
		\end{minipage}
	} \hfill{}
	\fbox{
		\begin{minipage}[c][50pt]{.45\textwidth}
			\ttfamily
			Answer the following question using the previous context in a few words and with no formatting.
		\end{minipage}
	}
	\caption{The prompts appended before the questions used for finding parametric and counterparametric data, respectively.}
	\label{prompts}
\end{figure}

\subsection{Generating and scoring parametric answers}

We query each of the models listed in \cref{models_and_resources} with the data from the previous subsections.

To ensure results are simple to interpret and not affected by randomness, we follow the example of Hsia~et.~al\cite{ragged} and use greedy decoding to find the answer.
While beam search with a short beam width tends to produce more accurate results for long answers\cite{sutskever_seq2seqlearning,wu_mltranslation} and there are many other sampling methods that produce better results\cite{text_degeneration}, this is likely to not have an effect on experiments that result in shorter answers\cite{t5}.

The cross-entropy of an answer $x$ is calculated in base of the conditional probability of generating each token given the prior tokens.
The perplexity, which measures how well a model predicts a sample, is calculated based on this cross-entropy.

\begin{equation}
	\begin{aligned}
		H \left( x_1, \dots, x_N \right) &= - \frac{1}{N} \sum^N_{i = 1} \log P \left( x_i \mid x_{i - 1}, \dots, x_1 \right) \\
		\text{Perplexity} \left( x_1, \dots, x_N \right) &= e^{H(x)}
	\end{aligned}
\end{equation}

\subsection{Shuffling to generate counterparametric answers}

Previous work related to finding per token probabilities of answers in large language models focus on either a pre-existing list of questions or on a single question format\cite{factual_recall}.
This approach does not work for our use case for three reasons.
\begin{enumerate}
	\item Having \baseqs{} different types of questions, rather than just 1, makes finding counterfactual answers technically challenging.
	\item Our focus is not on finding \emph{counterfactual} answers, but \emph{counterparametric} ones.
		We do not care about correctness; we care about answers not being parametric.
	\item Since we are measuring perplexity of these answers, we focus on answers that are generated by the same base question and the same model.
		This way we ensure that the format of the answer is the same.
\end{enumerate}

We propose a novel way of generating counterparametric answers while focusing on these three points: rather than generating new answers for each question, existing answers are grouped by base question and shuffled along with other objects to generate incorrect but identically-formatted answers.

\begin{table}[h]
	\newcommand{\vwidth}[1]{\begin{varwidth}{\columnwidth}#1\end{varwidth}}

	\centering
	\scriptsize

	\begin{tabular}{l l l l}
		\toprule
			\bfseries Base Question & \bfseries Parametric & \bfseries Counterparametric & \bfseries Reformed Question \\
		\midrule
			\vwidth{What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ The date of birth of \textcolor{Red}{Che~Guevara} is} & June 14, 1928 \\
			\vwidth{What is the date of birth of Ibn al-Haytham? \\ The date of birth of Ibn al-Haytham is} & 965 AD \\
			\vwidth{What is the date of birth of Boyan Slat? \\ The date of birth of Boyan Slat is} & 27 January 1994 \\
			\vwidth{What is the date of birth of W.E.B. Du Bois? \\ The date of birth of W.E.B. Du Bois is} & February 23, 1868 \\
			\vwidth{What is the date of birth of Stephen Hawking? \\ The date of birth of Stephen Hawking is} & January 8, 1942 \\
			\vwidth{What is the date of birth of Shirin Ebadi? \\ The date of birth of Shirin Ebadi is} & June 21, 1947 \\
		\bottomrule
	\end{tabular}
\end{table}
