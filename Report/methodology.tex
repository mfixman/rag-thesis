\section{Methods}
\label{methods_section}

How does a large language model respond when given context that contradicts its parametric, learned knowledge? Why does it choose this answer?

To understand this, we build a new framework for testing a large language model's answers when presented with contradictory information.
We test this framework with models of various architectures and sizes to get insights about our responses.

Following the example set in \cref{introduction_research_objectives}, we split this work into three sub-objectives.

\subsection{Creating a representative dataset of questions}
\label{creating_dataset}

As argued in \cref{questions_objective}, the research of this thesis requires a large dataset of questions from a variety of categories to test large language models.

\subsubsection{Dataset Description}

The dataset we aim to create for this research is designed to be a comprehensive and versatile tool for evaluating large language models.
By selecting a wide variety of questions questions we ensure that the dataset will provide meaningful insights of the grounding of LLMs across a wide spectrum of domains.

Our dataset should have the following properties.
\begin{description}[style=nextline,topsep=-5pt]
	\item[1. Questions should have short, unambiguous answers.]
		Our goal is to compare these answers both for equality, on the LLM's perplexity at generating this result. Longer answers make this objective harder to interpret since two long answers might have a higher chance of being both correct. Ensuring our answers are short reduces the space of different but equivalent answers.
	\item[2. Questions must cover a large and diverse set of topics.]
		Parametric answers are sourced from the training data of a large language model, which might be biased towards certain topics or groups of people. For example, it is known that Wikipedia contains a significant geographical bias on biographies \citep{wikipedia_geographic_bias}, and that this affects the probability of choosing to answer from parametric or contextual knowledge\citep{factual_recall}.
		We require a large and diverse and set of topics to counteract potential biases.
	\item[3. Questions must allow for the creation of counterparametric answers.]
		In order to test a model's knowledge grounding, this thesis requires understanding whether an answer came from contextual versus inherent knowledge.
	    A simple way to do this is to repeat and enhance the approach used by \citeauthor{factual_recall} of adding counterparametric answers to a query context.
		This allows us to to easily disambiguate whether an answer came from the model's inherent parametric memory or from the given context.
	    This approach is only possible if the set of answers allows us to create a set of alternative answers that are plausibly correct and have the same format as the parametric answer, but are still counterparametric.
\end{description}

The existing literature uses various existing question-and-answer datasets.
We believe that none of these datasets are a good fit for this research due to not following some of the three desired properties.
However, understanding them can be useful when designing the final dataset.

\begin{description}
	\item[Natural Questions Dataset] Created by Google Research \citep{natural_questions}, and commonly used in research related to understanding the answers of LLMs in question-and-answer problems \citep{ragged,when_not_to_trust_llms,can_rag_models_reason}.
		While the dataset provides an excellent range of questions and existing literature to compare these results to, the lack of question categorisation is an obstacle in our objective to generate counterparametric answers.
	\item[Human-Augmented Dataset] Sometimes used in research related to quality control of large language models \citep{learning_the_difference}.
		However, the high cost associated in generating this dataset would limit the size of our questions.
	\item[Countries' Capitals Question Dataset] Used in ``Characterizing Mechanisms for Factual Recall in Language Models'' \citep{factual_recall}, this dataset contains a single question about the capital city of certain countries which can be easily transformed to a counterparametric question.
		This format is ideal for the research done in this thesis, but having a single type of question will not allow a deep dive into the source of each answer in a general question.
\end{description}

\subsubsection{Dataset Creation}

Instead of using an existing dataset, this research takes inspiration from countries' capital queries used in the paper by \citeauthor{factual_recall}, and creates a similar but larger and more varied dataset of questions and answers from a wide range of topics, assuring questions can be grouped by question pattern so that the formats of their answer are similar.
This way, we can emulate the approach used in that paper of reusing the answer from a certain question as the counterfactual context of another.

This dataset will be used for the experiments of this thesis.
Since it might be useful for future research, it will also be presented as its own result.

Since this thesis requires a set of questions that covers a large set of topics, we first generate by hand a list of various categories of questions.
Each one of these categories refers to the subject of the question, rather than the answer.

For each category we create a set of base questions and another set of objects.
We ensure that for all the objects, and specially in the case of people, the resulting list of objects represents a wide variety of objects from across the world

For each category, we generate a set of questions by matching every one of its base questions with every one of its objects.
An example of this approach is shown in \cref{source_data_example}.

\begin{table}[htb]
	\setlength{\fboxsep}{0pt}
	\setlength{\fboxrule}{1pt}
	\newcommand{\rep}[1]{{\setlength{\fboxsep}{0pt}\fcolorbox{Gray}{Gray!80}{\textit{#1}}}}

	\centering
	\scriptsize
	\begin{tabular}{>{\bfseries}c | l | c | l}
		\toprule
			\bfseries Category & \bfseries Base Questions & \bfseries Object & \bfseries Queries \\
		\midrule
			Person & \begin{minipage}{.30\textwidth}
				\ttfamily
				Q: What is the date of birth of \rep{\{person\}}? \\ A: The date of birth of \rep{\{person\}} is \\[1ex]
				Q: In what city was \rep{\{person\}} born? \\ A: \rep{\{person\}} was born in
			\end{minipage} &
			\begin{minipage}{.12\textwidth}
				\ttfamily
				\textcolor{Red}{Che~Guevara} \\[1ex]
				\textcolor{Sepia}{Confucius}
			\end{minipage} &
			\begin{minipage}{.40\textwidth}
				\ttfamily
				Q: What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ A: The date of birth of \textcolor{Red}{Che~Guevara} is \\[1ex]
				Q: What is the date of birth of \textcolor{Sepia}{Confucius}? \\ A: The date of birth of \textcolor{Sepia}{Confucius} is \\[1ex]
				Q: In what city was \textcolor{Red}{Che~Guevara} born? \\ A: \textcolor{Red}{Che~Guevara} was born in \\[1ex]
				Q: In what city was \textcolor{Sepia}{Confucius} born? \\ A: \textcolor{Sepia}{Confucius} was born in
			\end{minipage} \\
		\midrule
			City & \begin{minipage}{.30\textwidth}
				\ttfamily
				Q: What country is \rep{\{city\}} in? \\ A: \rep{\{city\}} is in
			\end{minipage} &
			\begin{minipage}{.10\textwidth}
				\ttfamily
				\textcolor{BurntOrange}{Cairo} \\[1ex]
				\textcolor{ForestGreen}{Mumbai} \\[1ex]
				\textcolor{Cyan}{Buenos Aires} \\[1ex]
				\textcolor{Purple}{London}
			\end{minipage} &
			\begin{minipage}{.40\textwidth}
				\ttfamily
				Q: What country is \textcolor{BurntOrange}{Cairo} in? \\ A: \textcolor{BurntOrange}{Cairo} is in \\[1ex]
				Q: What country is \textcolor{ForestGreen}{Mumbai} in? \\ A: \textcolor{ForestGreen}{Mumbai} is in \\[1ex]
				Q: What country is \textcolor{Cyan}{Buenos Aires} in? \\ A: \textcolor{Cyan}{Buenos Aires} is in \\[1ex]
				Q: What country is \textcolor{Purple}{London} in? \\ A: \textcolor{Purple}{London} is in
			\end{minipage} \\
		\bottomrule
	\end{tabular}
	\caption{Some examples of the base-question and object generation that are fed to the models for finding parametric answers. For each category, we match every base question to every object to create a longer set of questions. In this example, the \textit{Person} category contains 2 base questions and 2 objects, resulting in 4 questions; the \textit{City} category contains 1 base question and 4 objects, also resulting in 4 questions.}
	\label{source_data_example}
\end{table}

This list of questions will enable the research on whether the answers given by large language models depend on the category and the format of the questions.

\subsection{Building an experimental framework to understand the source of an LLM's answer}
\label{method22}

\subsubsection{Model Selection}
\label{model_selection}

In order to understand the knowledge grounding of a wide variety of large language models, the queries generated in \cref{creating_dataset} are tested with four models of different architectures and sizes.
These models are listed in \cref{model_list}.

\begin{table}[htb]
	\centering
	\begin{tabular}{>{\bfseries}c@{\hspace{20pt}}l l}
		\toprule
			& \bfseries Seq2Seq Model & \bfseries Decoder-Only Model \\
		\midrule
			Smaller & \ttfamily Flan-T5-XL & \ttfamily Meta-Llama-3.1-8B-Instruct \\
			Larger & \ttfamily Flan-T5-XXL & \ttfamily Meta-Llama-3.1-70B-Instruct \\
		\bottomrule
	\end{tabular}
	\caption{The four large language models chosen for this research.}
	\label{model_list}
\end{table}

All of the models used in this research leverage autoregressive attention using the transformer architecture \citep{attention_is_all_you_need}, where each token attends to its preceding tokens, maintaining the temporal order of the sequence.
This approach allows them to generate coherent and contextually relevant text by sampling from this learned distribution, while also capturing long-range dependencies and complex patterns in language.

Both Sequence-to-Sequence models are based on T5 models \citep{t5}, which employ an encoder-decoder architecture: while an encoder processed the input sequence into a context vector, and an decoder generates an input sequence from this vector.
The \texttt{Flan-T5} models are fine-tuned to follow instructions, and have improved zero-shot performance compared to the original T5 models \citep{flant5}.

\texttt{Flan-T5-XL} contains approximately 3 billion parameters.
This is considerably bigger than the base Flan-T5 model \citep{flant5}, which will provide better accuracy of its parametric answers.

\texttt{Flan-T5-XXL} contains 11 billion parameters, has higher accuracy on the parametric answers as the \texttt{XL} model \citep{flant5}.
However, how the higher amount of parameters will affect its knowledge grounding when running our experiment is still unknown.

Decoder-only models generate answers one token at a time from the input query.
Given a sequence of tokens, they generate text one token at a time by attempting to solve the problem of predicting the following token \citep{gpt}.

This thesis uses the \texttt{-Instruct} versions of the latest Llama models \citep{llama3}, which use this architecture and fine-tune it to tasks of instruction-following.
These models are specially adept at complex prompts.
Of the models used in this thesis, \texttt{Meta-Llama-3.1-8B-Instruct} has 8 billion parameters, while \texttt{Meta-Llama-3.1-70B-Instruct} has 70 billion.

The properties of the models are summarised in \cref{model_card}.

\begin{table}[htbp]
	\centering
	\footnotesize
	\begin{tabular}{>{\ttfamily}l@{\hspace{3pt}}l@{\hspace{3pt}}l@{\hspace{3pt}}r@{\hspace{3pt}}r}
		\toprule
			\rmfamily \bfseries Model & \bfseries Architecture & \bfseries Trained On & \bfseries Parameters \\
		\midrule
			Flan-T5-XL & Seq2Seq & Public NLP Datasets & 3 Billion \\
			Flan-T5-XXL & Seq2Seq & Public NLP Datasets & 11 Billion \\[2pt]
			Meta-Llama-3.1-8B-Instruct & Decoder-Only & Web text, code, instruction datasets & 8 Billion \\
			Meta-Llama-3.1-70B-Instruct & Decoder-Only & Web text, code, instruction datasets & 70 Billion \\
		\bottomrule
	\end{tabular}
	\caption{Model cards of the large language models used in this research.}
	\label{model_card}
\end{table}

\subsubsection{Understanding the source of the answer in each model}
\label{methodology_type_of_answer}

The first step to understanding the knowledge grounding of large language models is to create queries that contain data that contradicts its parametric knowledge as part of the context.
By comparing the result to the existing answers it becomes trivial to understand whether an answer came from the model's memory, the queries' context, or neither of these.

Following the approach done by \citeauthor{factual_recall}, for every query we randomly sample from the set of answers of the same base question for answers that are different to the parametric answer which is given by the original query.
An example of this random sampling can be found in \cref{counterparametric_table}.

\begin{table}[p]
	\newcommand{\vwidth}[1]{\parbox{38ex}{\ttfamily #1}}
	\newcommand{\rep}[1]{{\setlength{\fboxsep}{0pt}\fcolorbox{Gray}{Gray!80}{\textit{#1}}}}

	\centering
	\scriptsize

	\begin{tabularx}{\textwidth}{>{\ttfamily}l>{\ttfamily}c@{\hspace{1pt}}>{\ttfamily}c@{\hspace{0pt}}>{\ttfamily}c@{\hspace{10pt}}>{\ttfamily}X}
		\toprule
			\rmfamily \bfseries Base Question & \rmfamily \bfseries Object & \rmfamily \bfseries \parbox{40pt}{\centering Parametric Answer} & \rmfamily \bfseries \parbox{75pt}{\centering Counterparametric Answer} & \rmfamily \bfseries \parbox{120pt}{\centering Question with Counterparametric Context} \\
		\midrule
			\multirow{4}{65pt}[-45pt]{Q: What is the date of birth of \protect\rep{\{person\}}? \\ A: The date of birth of \protect\rep{\{person\}} is} &
			\textcolor{Red}{Che~Guevara} &
			\textcolor{Red}{June~14,~1928} &
			\textcolor{Apricot}{965~AD} &
			\vwidth{Context: [the date of birth of \textcolor{Red}{Che~Guevara} is \textcolor{Apricot}{965~AD}]. \\ Q: What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ A: The date of birth of \textcolor{Red}{Che~Guevara} is} \vspace{2pt} \\
			%
			&
			\textcolor{Apricot}{Ibn~al-Haytham} &
			\textcolor{Apricot}{965~AD} &
			\textcolor{Red}{June~14,~1928} &
			\vwidth{Context: [the date of birth of \textcolor{Apricot}{Ibn~al-Haytham} is \textcolor{Red}{June~14,~1928}]. \\ Q: What is the date of birth of \textcolor{Apricot}{Ibn~al-Haytham}? \\ A: The date of birth of \textcolor{Apricot}{Ibn~al-Haytham} is} \vspace{2pt} \\

			&
			\textcolor{Blue}{Boyan~Slat} &
			\textcolor{Blue}{27~January~1994} &
			\textcolor{Brown}{February~23,~1868} &
			\vwidth{Context: [the date of birth of \textcolor{Blue}{Boyan~Slat} is \textcolor{Brown}{February~23,~1868}]. \\ Q: What is the date of birth of \textcolor{Blue}{Boyan~Slat}? \\ A: The date of birth of \textcolor{Blue}{Boyan~Slat} is} \vspace{2pt} \\

			&
			\textcolor{Brown}{W.E.B~Du~Bois} &
			\textcolor{Brown}{February~23,~1868} &
			\textcolor{Red}{June~14,~1928} &
			\vwidth{Context: [the date of birth of \textcolor{Brown}{W.E.B~Du~Bois} is \textcolor{Red}{June~14,~1928}]. \\ Q: What is the date of birth of \textcolor{Brown}{W.E.B~Du~Bois}? \\ A: The date of birth of \textcolor{Brown}{W.E.B~Du~Bois} is} \vspace{2pt} \\
			%
		\midrule
			\multirow{2}{65pt}[-10pt]{Q: What country is \protect\rep{\{city\}} in? \\ A: \protect\rep{\{city\}} is in}
			&
			\textcolor{BurntOrange}{Cairo} &
			\textcolor{BurntOrange}{Egypt} &
			\textcolor{ForestGreen}{India} &
			\vwidth{\vspace{2pt} Context: [\textcolor{BurntOrange}{Cairo} is in \textcolor{ForestGreen}{India}]. \\ Q: What country is \textcolor{BurntOrange}{Cairo} in? \\ A: \textcolor{BurntOrange}{Cairo} is in} \vspace{2pt} \\
			%
			&
			\textcolor{ForestGreen}{Mumbai} &
			\textcolor{ForestGreen}{India} &
			\textcolor{BurntOrange}{Egypt} &
			\vwidth{Context: [\textcolor{ForestGreen}{Mumbai} is in \textcolor{BurntOrange}{Egypt}]. \\ Q: What country is \textcolor{ForestGreen}{Mumbai} in? \\ A: \textcolor{ForestGreen}{Mumbai} is in} \vspace{2pt} \\
		\bottomrule
	\end{tabularx}
	\caption{Using the same question format allows us to repurpose previous parametric answers as counterparametric ones. In this example, we randomly sample answers from the same base question but referring to a different object to find counterparametric answers, which we later add to the context of the query.}
	\label{counterparametric_table}
\end{table}

% \newpage{}
We later add this \emph{counterparametric answer} to the context, to form a new query and query the same model again with the added counterparametric context.
This is exemplified in \cref{category_example}.

To ensure that the results are simple to interpret and minimise the effect of randomness, once we select the queries we follow the example of \citeauthor{ragged} and use Greedy Decoding to generate the answer.
While beam search with tends to produce more accurate results for long answers \citep{sutskever_seq2seqlearning,wu_mltranslation} and there are many other sampling methods that tend to produce better results \citep{text_degeneration}, this is likely to not have an effect on experiments shorter answers \citep{t5}.

We compare the generated answer with the context to the previously generated parametric answer, and we categorise the answer:
\begin{description}
	\item[\Parametric{}] answers are equal to the answer given by the model when queried without context.
		This answer would come from the parametric memory of the model, and could potentially indicate an hallucination not present in the context.
	\item[\Contextual{}] answers are equal to the context given in the query.
		In a RAG context, this would be the answer retrieved from the index.
	\item[\Other] answers are neither of these, and this answer comes from a mis-interpretation of the input by the model or from some other source.
\end{description}

To minimise the amount of problems caused by large language models generating extra information, we define two strings to be equal by truncate the text until the first period or \texttt{<EOS>} token, removing punctuation and stop words, and finding whether one of the answers is a subsequence of another.

We later found out that this approach to compare strings could be improved, and understanding whether two generated answers are identical is an ongoing problem.
This is later explained in the Future~Work section in \cref{other_problems}.

\begin{figure}[tb]
	\centering
	\fbox{\includegraphics[width=\textwidth]{Method.png}}
	\caption{Example diagram of steps taken to calculate the sets of \Parametric{}, \Contextual{}, and \Other{} answers. The terms in this diagram are explained in the \protect\hyperref[glossary]{Glossary}.
	The following steps are shown: \\
		\phantom{.}\hspace{5pt} 1. Generate questions by combining every base question with every object of the same category. \\
		\phantom{.}\hspace{5pt} 2. Query each model, and get a set of parametric answers. \\
		\phantom{.}\hspace{5pt} 3. Shuffle these parametric answers along the same base question to get counterparametric answers. \\
		\phantom{.}\hspace{5pt} 4. Query the model again, using the counterparametric answers as context. \\
		\phantom{.}\hspace{5pt} 5. Group the answers according to their source.
	}
	\label{action_diagram}
\end{figure}

\begin{table}[h]
	\centering
	\scriptsize

	\begin{tabularx}{\textwidth}{>{\ttfamily}X >{\ttfamily}c c}
		\toprule
			\bfseries \rmfamily Question with counterparametric context & \bfseries \rmfamily Model Answer & \bfseries Category \\
		\midrule
			\parbox{235pt}{Context: [the nearest major body of water to \textcolor{Mahogany}{Windhoek} is the \textcolor{RoyalPurple}{Rio de la Plata}] \\ Q: What is the nearest major body of water to \textcolor{Mahogany}{Windhoek}? \\ A: The nearest major body of water to \textcolor{Mahogany}{Windhoek} is} &
			\textcolor{Mahogany}{the Atlantic Ocean} &
			\bfseries \textcolor{ForestGreen}{Parametric} \\[22pt]
			%
			\parbox{235pt}{Context: [the date of birth of \textcolor{Red}{Che~Guevara} is \textcolor{Apricot}{965~AD}]. \\ Q: What is the date of birth of \textcolor{Red}{Che~Guevara}? \\ A: The date of birth of \textcolor{Red}{Che~Guevara} is} &
			\textcolor{Apricot}{965~AD} &
			\bfseries \textcolor{Maroon}{Contextual} \\[16pt]
			%
			\parbox{235pt}{Context: [\textcolor{Purple}{Rome} is in \textcolor{Salmon}{Georgia}] \\ Q: What country is \textcolor{Purple}{Rome} in? \\ A: \textcolor{Purple}{Rome} is in} &
			\textcolor{BlueViolet}{the United States} &
			\bfseries \textcolor{MidnightBlue}{Other} \\[12pt]
		\bottomrule
	\end{tabularx}
	\caption{Example for results for three questions. We enhance each question with a context containing data that is different to the answer given by the model for this question. We later categorise the source of the answer as \Parametric{} if it came from the model's inherent memory, \Contextual{} if it came from the context, or \Other{} if it's neither of these. Note that, in the third query, the model is interpreting the question as asking about Rome in the US State of Georgia, rather than the country of Georgia.}
	\label{category_example}
\end{table}

\clearpage{}

\subsubsection{Understanding the result by finding the mean attention of the context and question areas}
\label{attention_section}

When comparing different architectures, it's useful to understand how much attention they give to the context compared to the rest of the query.

Given that all our model architectures employ attention mechanisms \citep{flant5,llama3}, we can estimate the relative importance of the tokens in each section of the query by calculating the average self-attention each token receives within its respective section.
Specifically, we compute the mean self-attention weights, which serve as a proxy for the emphasis the model places on different parts of the input.

This approach is formalized in \cref{attention_equation}, where we define the mean self-attention for tokens in the context and query sections.

\begin{equation}
	\newcommand{\lenmod}[1]{\left| \text{#1} \right|}
	\begin{gathered}
		A : \mathbb{R} ^ { \text{batch} \times \lenmod{layer}  \times \lenmod{attn\_head} \times Q \times K } \\[1ex]
		\begin{aligned}
			m_{b,q,k} &= \frac{1}{ \lenmod{layer} \times \lenmod{attn\_head} } \sum^{\substack{\text{layer} \\ \text{attn\_head}}}_{\substack{i = 0 \\ j = 0}} A_{b,i,j,q,k} \\
			d_{b, q} &= m_{b, q, q} \\
			s_{b, i} &= \left(d_{b, i} - \min{\left( d_b \right)}\right) / \left(\max{\left( d_b \right)} - \min{\left( d_b \right)} \right)
		\end{aligned} \\[1em]
		\text{attn}_\text{ctx} = \frac{1}{\lenmod{ctx}} \sum_{i \in \text{ctx}} s_{b,i} \qquad
		\text{attn}_\text{rest} = \frac{1}{\lenmod{rest}} \sum_{i \in \text{rest}} s_{b,i} \qquad
	\end{gathered}
	\label{attention_equation}
\end{equation}

In this equation, $A$ is the 5-tensor representing the attention weights.
$m_{b,q,k}$ represents the mean attention all layers and heads, while $d_b$ represents the diagonal of these attentions which correspond to the self-attentions.

We normalise the values for the tokens in each query to $s_i$ in order to being able to compare them between different query, and average then among the context tokens and the rest of the query.
This results in two results that can be compared between different queries.

\newpage{}

\subsection{Enhancing the framework to understand the reasoning behind each answer}
\label{method_perplexity}

\subsubsection{Perplexity Score}
\newcommand{\NLL}{\text{NLL}}
\newcommand{\PPL}{\text{PPL}}

The Perplexity score of an answer is normally used to measure the inverse of the certainty that the model has of a particular answer \citep{gpt3,retro}.
In a sense, it's the ``surprise'' of a model that a certain answer is correct.

We can define the probability of a model choosing a token $x_n$ with context $x_1, \dots, x_{n - 1}$ from a query $Q$ by calculating the softmax value of all the logits for the possible words for this token.

The probabilities of of the answer generated from a query can be accumulated to calculate the negative log-likelihood $\NLL$, which is used to calculate the perplexity $\PPL$ using the formulas from \cref{eq:nll,eq:ppl}.

\begin{align}
	\NLL \left( x_1, \dots, x_n \mid Q \right) &= - \frac{1}{n} \sum^n_{i = 1} \log_2 P \left( x_i \mid Q, x_1, \dots, x_{i - 1} \right) \label{eq:nll} \\[1ex]
	\PPL \left( x_1, \dots, x_n \mid Q \right) &= {2 ^ {\text{NLL} \left( x_1, \dots, x_n \mid Q \right)}} \label{eq:ppl}
\end{align}

\subsubsection{Perplexity of the parametric answer with counterparametric context and vice-versa}

Note that, in these experiments, the token $x_n$ does not necessarily have to be the most likely result generated by the model when applying the query $x_1, \dots, x_{n - 1}$.

Therefore it becomes necessary to use teacher-forcing \citep{teacher_forcing} to feed some answer to the model regardless of what's the most likely answer for each successive token. This allows us to calculate the perplexity scores of the parametric answers for both the contextless query and the one with counterparametric context, and the perplexity scores of the contextual answers for these two queries.

For a given parametric answer $p_1, \dots, p_n$ and different counterparametric answer $q_1, \dots, q_m$, a query without context $Q$, and a query with this counterparametric context $Q'$ we can calculate four different perplexity scores $P_0, P_1, P_2, P_3$ as shown in \cref{perplexity_table}.

\begin{table}[hbt]
	\footnotesize
	\centering

	\renewcommand{\arraystretch}{3}
	\begin{tabular}{  l >{\centering}p{.2\textwidth} | >{\centering}p{.3\textwidth} | p{.3\textwidth} | }
		\cline{3-4}
			& & \multicolumn{2}{c|}{\raisebox{11pt}{\bfseries Tokens}} \\[-15pt]
		\cline{3-4}
			& & \raisebox{11pt}{Parametric $p$} & \raisebox{11pt}{\hspace{20pt} Counterparametric $q$} \\[-15pt]
		\hline
			\multicolumn{1}{ | c | }{\multirow[b]{2}{*}{\rotatebox{90}{\bfseries \centering  Context}}}
			& Base Query &
			$P_0 = \PPL \left( p_1, \dots, p_n \mid Q \right)$ &
			$P_1 = \PPL \left( q_1, \dots, q_{m} \mid Q \right)$ \\
		\cline{2-4}
			\multicolumn{1}{ | c | }{} & Counterparametric Context &
			$P_2 = \PPL \left( p_1, \dots, p_n \mid Q' \right)$ &
			$P_3 = \PPL \left( q_1, \dots, q_{m} \mid Q' \right)$ \\
		\hline
	\end{tabular}
	\caption{We calculate four different perplexity values: one for each set of tokens, and one for each query context. We care about $P_2$ and $P_3$, which are the perplexities at getting the parametric and counterfactual answers in a query with counterfactual context.}
	\label{perplexity_table}
\end{table}

Since the parametric answer is by definition the response of the model to the regular query, $P_0 \leq P_1$ and the perplexity of the parametric value is lower than the perplexity of any other answer on query $Q$.

\Cref{example_perplexity} contains an example of the calculation of the perplexity values for a particular query in a case where the contextual answer is considerably less surprising than the parametric answer.

Similar to the work done earlier in this section, we say that the perplexity of an answer is \Parametric{} if the probability of getting the parametric answer is greater than the probability of getting the contextual answer; that is, $P_2 < P_3$.
Contrary to this, we consider an answer \Contextual{} if $P_2 > P_3$.

For simplicity and to the myriad of possible token and probability combinations, in this experiment we do not analyse the case when an answer is preferred to either of these two.

\subsubsection{Predicting whether an answer came from memory or from context}

One question remains: if the response of the query with counterparametric context $Q'$ is a certain answer $x_1, \dots, x_n$, can we predict whether this answer is came from the model's memory $p$ or from the given context $q$ without requiring an extra query?

We propose investigating the value of the perplexity $\PPL \left( x_1, \dots, x_n \mid Q' \right)$ and comparing it to the distribution of perplexities on queries with added counterparametric context $P_2$ and $P_3$.
The probability of this value being on one distribution or another might give us a good idea of the source the model used for generating the answer.

\input{example_perplexity}
