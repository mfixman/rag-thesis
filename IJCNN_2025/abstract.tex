Large language models (LLMs) have seen significant advancements in quality and adoption, yet their tendency to hallucinate remains a critical issue in applications requiring precision.
Retrieval-Augmented Generation (RAG) has emerged as a promising solution to this problem, leveraging external information to enhance response accuracy.
However, it remains unclear when LLMs prioritize RAG-provided context over their internal parametric knowledge, raising questions about their reliability in knowledge grounding.

This paper investigates how LLMs respond to questions when contextual information contradicts with their parametric knowledge.
We develop a diverse dataset featuring questions across a wide variety of topics, and use model-generated answers to different but similar questions to create a counterparametric answers.
These are provided as part of the context of a new query, thus enabling us to determine whether the model sourced the answer to this question from the given context or its parametric memory.

We evaluate models of different architectures and sizes, including \smallflan{}, \bigflan{}, \smallllama{}, and \bigllama{}, analyzing their responses to determine their reliance on RAG context versus internal knowledge.
Our findings reveal that smaller models, and encoder-decoder Seq2Seq models exhibit stronger knowledge grounding and fewer hallucinations compared to larger models and decoder-only models.
This analysis could help develop strategies to mitigate hallucinations in RAG-augmented LLMs, ultimately improving their reliability in knowledge-intensive tasks.
