Large language models (LLMs) have seen significant advancements in quality and adoption, yet their tendency to hallucinate remains a critical issue in applications requiring precision.
Retrieval-Augmented Generation (RAG) has emerged as a promising solution to this problem, leveraging external information to enhance response accuracy.
However, it remains unclear when LLMs prioritize RAG-provided context over their internal parametric knowledge, raising questions about their reliability in knowledge grounding.

This paper investigates how LLMs respond to questions when contextual information contradicts with their parametric knowledge.
We develop a diverse dataset featuring questions across a wide variety of topics, and use model-generated answers to different but similar questions to create a counterparametric answers.
These are provided as part of the context of a new query, thus enabling us to determine whether the model sourced the answer to this question from the given context or its parametric memory.

We evaluate different architectures and sizes, including Seq2Seq encoder-decoder models \smallflan{} and \bigflan{} and decoder-only models \smallllama{} and \bigllama{}, analyzing their responses to determine their reliance on RAG context versus internal knowledge.
Our findings reveal that encoder-decoder Seq2Seq models tend to exhibit stronger knowledge grounding than decoder-only models.
Additionally, smaller decoder-only models tend to outperform larger ones in this area.

This analysis could help develop strategies to mitigate hallucinations in RAG-augmented LLMs, ultimately improving their reliability in knowledge-intensive tasks.
