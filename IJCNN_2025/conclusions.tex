\section{Conclusions}

We presented an empirical study on how knowledge grounding in large language models, probing how they respond when provided with contradictory context.
Our experiments show that encoder-decoder architectures and smaller models better integrate new evidence, while large decoder-only models often revert to their parametric knowledge.
Additionally, we show model size does not have a large impact on encoder-decoder models, while larger Decoder-only models tend to have a higher rate of answers answers that sourced from their parametric knowledge.

Answers that are neither \Parametric{} nor \Contextual{} tend to have a source that follows a similar distribution for these models.

These insights can inform the selection of models and inference strategies for tasks where factual accuracy is important and the contextual knowledge added to the query is more reliable than the parametric knowledge of the LLM.

These findings carry implications for models using retrieval-augmented generation that get factual contextual data from an index: if external context is more trustworthy than a model's memorised content, then an encoderâ€“decoder setup or smaller decoder-only models may be preferable.
Conversely, larger decoder-only models might offer greater breadth of knowledge but risk overriding crucial contextual updates.
% This is particularly useful in the framework of question-answering when using Retrieval-Augmented Generation (RAG) to fetch contextual data from an index.
Understanding the knowledge grounding of LLMs is crucial for preventing hallucinations when enhancing queries with a RAG-generated index.

\subsection{Future Work}

\subsubsection{Better categorisation of \Other{} answers}
A cursory glance to the answers marked as \Other{} in \cref{what_are_all_these_others} shows that several of them came from the model's \Parametric{} knowledge (categories 1 and 6, which represent $3.79\%$ and $5.40\%$ of the answers in decoder-only models) and others from the \Contextual{} data in the query (category 5, which represents $5.03\%$ and $3.59\%$ of the answers in the encoder-decoder-models).
The algorithm used to understand the source of the knowledge of an answer is currently simple, and could be improved to provide better understanding.

\subsubsection{Knowledge grounding in retrieval-augmented LLMs}
Running this program on retrieval-augmented LLMs such as \textsc{Atlas}~\cite{atlas_foundational} and \textsc{Retro}~\cite{retro} and creating a full evaluation framework that specifically focuses on their grounding might help understanding the capability of these models to adapt their generation to their provided context.

\subsubsection{Using perplexity data to guide the answers}
The perplexity of a response has been used to understand and calibrate the knowledge grounding of large language models~\cite{how_can_we_know}.
Using it as an extra parameter of this research might provide extra insights.

Overall, we can take a step closer to building more trustworthy and reliable language models by deepening our understanding of knowledge grounding.
