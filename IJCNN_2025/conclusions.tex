\section{Conclusions}

We presented an empirical study on knowledge grounding in LLMs, probing how models respond when provided with contradictory context.
We showed that Seq2Seq architectures and smaller models better integrate new evidence, while large decoder-only models often revert to their \Parametric{} knowledge.  Additionally, model size does not have a large impact on Seq2Seq models, while it tends to push answers in Decoder-only models towards answers that use more \Parametric{} knowledge.

Answers that are neither \Parametric{} nor \Contextual{} tend to have a source that follows a similar distribution for these models.

These insights can inform the selection of models and inference strategies for tasks where factual accuracy is crucial, and will be helpful on models which add a RAG to ensure that the model's parametric knowledge does not override it.

By deepening our understanding of knowledge grounding, we take a step closer to building more trustworthy and reliable language models.
