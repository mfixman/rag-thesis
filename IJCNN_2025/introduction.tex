\section{Introduction}

Large language models have become central to many NLP applications, such as question answering \cite{gpt3,how_can_we_know}, reasoning tasks \cite{treeofthoughts}, and code generation.
Despite their impressive capabilities, hallucinations continue to pose serious problems by outputting factually incorrect outputs with a tone of high confidence \cite{how_can_we_know}.
For tasks where precision is paramount, such as factual QA or medical and legal domains, reducing hallucinations is critical.

Retrieval-augmented generation (RAG) \cite{rag} aims to mitigate hallucinations by supplying relevant context from an external index.
In principle, providing accurate and verifiable text at inference time should guide the model toward correct answers.
However, even with the addition of a context generated by RAG, LLMs may override provided evidence with their parametric knowledge.
This is especially common when the context contradicts the model's knowledge \cite{factual_recall,ragged}.

This phenomenon relates to \emph{knowledge grounding}: how well a model integrates external context into its response.
Recent studies show that factors such as model architecture, size, and training method influence this interplay \cite{factual_recall,flant5,llama}.
Yet, it remains unclear under what conditions LLMs override their intrinsic knowledge in favor of given context.

We create a diverse dataset of short-answer questions from broad topics (people, cities, principles, elements) and test LLM responses both without and with counterparametric contextâ€”statements that contradict the model's known answer.
We examine four models: two encoder-decoder (\smallflan{}, \bigflan{}) \cite{t5,flant5} and two decoder-only models (\smallllama{}, \bigllama{}) \cite{llama3}.
This paper presents an empirical study of knowledge grounding by answering questions from a broad range of topics and testing the answer of an LLM when presented with counterparametric context that contradicts the model's known answer.
By systematically injecting this contradictory context, we observe whether the model chooses the \Contextual{} answer from the prompt, a \Parametric{} answer from its ground memory, or some \Other{} answer that's different to both.

Our findings show that encoder-decoder models and smaller models rely more on context, significantly reducing hallucinations in contradictory scenarios.
Larger decoder-only models tend to ignore contradictory evidence and revert to their parametric knowledge.

This study contributes to a deeper understanding of knowledge grounding in LLMs, offering insights for designing more reliable RAG systems.
By choosing architectures that better incorporate given context, developers can reduce undesired hallucinations.
Ultimately, improving knowledge grounding is vital for building more trustworthy language models for knowledge-intensive tasks.
