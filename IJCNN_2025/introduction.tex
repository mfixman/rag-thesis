\section{Introduction}

Large language models have become central to many NLP applications, such as question answering~\cite{gpt3,how_can_we_know}, reasoning tasks~\cite{treeofthoughts}, and code generation\cite{alphacode_generation}.
Despite their impressive capabilities, hallucinations continue to pose serious problems by outputting factually incorrect outputs with a tone of high confidence~\cite{how_can_we_know}.
For tasks where precision is paramount, such as factual QA or medical and legal domains, reducing hallucinations is critical\cite{mitigating_hallucinations}.

Retrieval-augmented generation (RAG)~\cite{rag} aims to mitigate hallucinations by supplying relevant context from an external index.
In principle, providing accurate and verifiable text at inference time should guide the model toward correct answers.
However, even with the addition of a context generated by RAG, LLMs may override provided evidence with the parametric knowledge coming from their training data.
This is especially common when the context disagrees with the model's knowledge~\cite{factual_recall,ragged}.

This phenomenon relates to \emph{knowledge grounding}: how well a model ensures its answers are based on external, verifiable sources rather than solely relying on parametric memory~\cite{rag}.
Recent studies show that factors such as model architecture, size, and training method influence this interplay~\cite{factual_recall,flant5,llama}.
Yet, it remains unclear under what conditions LLMs choose to source their answer from the query's context or from the model's knowledge.

The knowledge grounding of a model might be affected by the properties of that model, such as its architecture and size, and on the topic and context of the question.
Any analysis must incorporate several models with a questions from a variety of topics.

This study contributes to a deeper understanding of knowledge grounding in large language models, offering insights for designing more reliable RAG systems.
By choosing architectures that better incorporate given context, developers can reduce undesired hallucinations.

Ultimately, understanding the knowledge grounding of large language models is vital for building more trustworthy language models for knowledge-intensive tasks.
