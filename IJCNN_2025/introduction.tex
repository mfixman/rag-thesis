\section{Introduction}

Large language models have become central to many NLP applications, such as question answering~\cite{gpt3,how_can_we_know}, reasoning tasks~\cite{treeofthoughts}, and code generation\cite{alphacode_generation}.
Despite their impressive capabilities, hallucinations continue to pose serious problems by outputting factually incorrect statements with a tone of high confidence~\cite{how_can_we_know}.
For tasks where precision is paramount, such as factual QA or medical and legal domains, reducing hallucinations is critical\cite{mitigating_hallucinations}.

\emph{Knowledge grounding} refers to the extent to which a language model bases its responses on external, verifiable sources rather than relying solely on the parametric knowledge encoded in its weights. 
A well-grounded model should prioritise provided context when available, ensuring that its outputs align with the given evidence rather than hallucinated or outdated information. 
This concept is particularly important in applications requiring factual accuracy, as models that fail to properly ground their responses may generate misleading or incorrect information despite being given reliable context~\cite{rag,factual_recall}.
Understanding how different models balance parametric and contextual knowledge is important for improving their reliability in real-world scenarios.

Retrieval-augmented generation (RAG)~\cite{rag} aims to mitigate hallucinations by supplying relevant context from an external index.
In principle, providing accurate and verifiable text at inference time should guide the model toward correct answers.
However, even with the addition of a context generated by RAG, LLMs may override provided evidence with the parametric knowledge coming from their training data, %.
%This is especially common 
when the context disagrees with the model's knowledge~\cite{factual_recall,ragged}.

Understanding how well a model follows in its answers provided external, verifiable sources rather than solely relying on parametric memory~\cite{rag} can help improve the knowledge grounding of RAG with LLMs. 
Recent studies show that factors such as model architecture, size, and training method influence this interplay~\cite{factual_recall,flant5,llama}.
Yet, it remains unclear under what conditions LLMs choose to source their answer from the query's context or from the model's knowledge.

This research attempts to answer the following key question: \textbf{How does an LLM respond when given information that contradicts its learned parametric knowledge, and why?}

% To answer this question we create a diverse dataset of short-answer questions from a broad range of topics, including people, cities, scientific principles, and others.
% This dataset is used on a variety of large language models to find the \emph{parametric answer}; later answers from different questions are used to find a \emph{counterparametric answer}.
% This answer is added to the context of a new query to understand how each model treats this knowledge.

To achieve this, we present an empirical study of knowledge grounding by answering questions from a broad range of topics and testing the answer of an LLM when presented with counterparametric context that contradicts the model's known answer.
By systematically injecting this contradictory context, we observe whether the model chooses the \Contextual{} answer from the prompt, a \Parametric{} answer from its ground memory, or some \Other{} answer that's different to both.

% Our findings show that encoder-decoder models and smaller models rely more on context, significantly reducing hallucinations in contradictory scenarios.
% Larger decoder-only models tend to ignore contradictory evidence and revert to their parametric knowledge.

This study contributes to a deeper understanding of knowledge grounding in large language models, offering insights for designing more reliable RAG systems.
By choosing architectures that better incorporate given context, developers can reduce undesired hallucinations.

Ultimately, understanding the knowledge grounding of large language models is vital for building more trustworthy language models for knowledge-intensive tasks.
