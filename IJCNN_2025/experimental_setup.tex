\section{Experimental Setup}

We design controlled experiments to test how LLMs handle contradictory context.
We first gather parametric answers from each model for a set of questions, then add counterparametric context and re-ask the questions.

\subsection{Dataset Creation}
We create a large, diverse dataset of short-answer questions spanning several domains: historical figures, cities, scientific principles, elements, books, paintings, events, buildings, and musical compositions.
These questions have known, short, and unambiguous answers, and are present in \cref{appendixA}.

\todo{Expand this section.}

\subsection{Knowledge Grounding Experimentation}
\input{counterparametric_example}

We follow the approach in \cite{factual_recall} to inject counterparametric context by taking an answer from one object of the same category and using it as contradictory context for another, as shown in \cref{counterparametric_example}.

We categorise the answer in three different groups. \todo{Ensure list in same page.}
\begin{enumerate}
	\item \Parametric{}: The answer is identical to the parametric answer, and comes from the parametric memory of the model.
	\item \Contextual{}: The answer is identical to the counterfactual answer, and comes from the model's context.
	\item \Other{}: The answer is something else, and can come from a combination of both answers or from something completely different.
\end{enumerate}

We evaluate four LLMs of different architectures and sizes, shown in \cref{models}.

\begin{table}[b]
	\centering
	\small
	\renewcommand{\arraystretch}{1.2}
	\begin{tabularx}{\columnwidth}{X l r}
		\toprule
			Model             & Architecture    & Params \\
		\midrule
			\smallflan{}      & Encoder-Decoder & 3B          \\[5pt]
			\bigflan{}        & Encoder-Decoder & 11B         \\[5pt]
			\llamaparbox{}    & Decoder-Only    & 8B          \\[10pt]
			\bigllamaparbox{} & Decoder-Only    & 70B         \\[10pt]
		\bottomrule
	\end{tabularx}
	\caption{Models evaluated in this study.}
	\label{models}
\end{table}

Flan-T5 \cite{flant5} is an instruction-tuned T5 model \cite{t5} with strong zero-shot capabilities.
Llama \cite{llama3} is a decoder-only architecture fine-tuned for instructions.
We compare two sizes for each one of these models to provide insights of knowledge grounding between different model types and sizes.
The full list of models can be found in \cref{models}.

For consistency and reproducibility, we use greedy decoding in all methods.
Additionally, spaces and special characters are stripped when comparing answers.

\subsection{Predicting parametric answers from perplexity data}
We can use the \emph{perplexity} of an answer to discover if it came from the model's parametric memory or from the query's context.
That is, whether it's a \Parametric{} or \Contextual{} answer.

To understand the internal confidence of a model, we use teacher-forcing to calculate the perplexity of both the \Parametric{} and the \Contextual{} answer with the counterfactual context added to the model.
Higher perplexity suggests the model finds the sequence less probable, offering a clue to whether an answer stems from parametric memory or from the provided context.

Studies of these values can be used to understand whether the perplexity of the real answer is closer to one of these two.

\subsection{Computational Resources}
All experiments were run on a server equipped with dual NVIDIA A100 GPUs (80GB VRAM each) and 48 CPU cores.
The A100â€™s large memory footprint allowed us to load and run the largest (70B) model efficiently.
