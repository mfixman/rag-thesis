\section{Methods}
\label{methodology_section}
\todo{Should this be ``Methods'' or ``Methodology''?}

This study investigates the behavior of large language models (LLMs) when presented with context that contradicts their parametric, learned knowledge.
To achieve this, we develop a comprehensive framework for evaluating the knowledge grounding of LLMs across different architectures and model sizes.

\subsection{Dataset Creation}
\label{dataset_creation}

\subsubsection{Rationale and comparison to prior datasets}

The foundation of this work is a representative dataset of questions designed to test the interplay between parametric and contextual knowledge in LLMs.
This dataset must satisfy three properties:
\begin{description}[style=nextline]
	\item[Short, unambiguous answers] Questions are constructed to elicit concise answers, enabling precise comparison and interpretation.
This avoids ambiguity and minimizes variability in answers, which is critical for identifying parametric versus contextual sources.
	\item[Coverage of diverse topics] The dataset spans a wide range of domains, from historical events to scientific concepts, to mitigate biases inherent in training data \cite{wikipedia_geographic_bias}.
	This diversity ensures a robust evaluation of grounding across different knowledge areas.
	\item[Conterparametric compatibility] Questions are designed to facilitate the addition of a context allowing an answer that contradicts the parametric answer.
\end{description}

Existing datasets, such as the Natural Questions dataset \cite{natural_questions} and the Countries' Capitals dataset \cite{factual_recall}, provided valuable insights but fell short of meeting all three criteria.
For example, while the Natural Questions dataset offers a wide range of questions, its lack of systematic categorization hinders counterparametric experiments.
The Countries' Capitals dataset, while well-suited for counterparametric evaluation, is limited in scope.

These limitations motivated the creation of a custom dataset.

\subsubsection{Dataset Design and Generation}

The design of this dataset is inspired by the methodology designed by Yu et al.\ \cite{factual_recall}.
In this paper, several queries of the form ``\texttt{What is the capital of \{country\}?}'' are asked and answers from different countries are used as counterfactual information.

This paper creates a similar but larger and more varied dataset of questions and answers from a wide range of topics, assuring questions can be grouped by question pattern so that the formats of their answer are similar.
This way we can emulate the approach used in that paper of reusing the answer from a certain question as the counterfactual context of another.

Our dataset consists of 9 different categories, each of which has a series of manually-written questions that can be answered with short and simple answers.

\subsection{Model Selection}
\label{model_selection}

In order to understand the knowledge grounding of a wide variety of large language
models, the queries generated in Section 3.1 are tested with four models of different
architectures and sizes.
These models are listed in \cref{model_list}.

% \begin{table}[htb]
% 	\centering
% 	\begin{tabular}{>{\bfseries}c l l}
% 		\toprule
% 			& \bfseries Seq2Seq Model & \bfseries Decoder-Only Model \\
% 		\midrule
% 			Smaller & \ttfamily Flan-T5-XL & \ttfamily Meta-Llama-3.1-8B-Instruct \\
% 			Larger & \ttfamily Flan-T5-XXL & \ttfamily Meta-Llama-3.1-70B-Instruct \\
% 		\bottomrule \addlinespace[4pt]
% 	\end{tabular}
% 	\caption{The four large language models chosen for this research.}
% 	\label{model_list}
% \end{table}

\begin{table}[b]
	\centering
	\footnotesize
	% \renewcommand{\arraystretch}{1.2}
	\begin{tabular}{p{100pt} l r}
		\toprule
			Model             & Architecture    & No of Params \\
		\midrule
			\smallflan{}      & Encoder-Decoder & 3B          \\[5pt]
			\bigflan{}        & Encoder-Decoder & 11B         \\[5pt]
			\llamaparbox{}    & Decoder-Only    & 8B          \\[10pt]
			\bigllamaparbox{} & Decoder-Only    & 70B         \\[10pt]
		\bottomrule \addlinespace[4pt]
	\end{tabular}
	\caption{Models evaluated in this study.}
	\label{model_list}
\end{table}

All of the models used in this research leverage autoregressive attention using the transformer architecture \cite{attention_is_all_you_need}, where each token attends to its preceding tokens, maintaining the temporal order of the sequence.
This approach allows them to generate coherent and contextually relevant text by sampling from this learned distribution, while also capturing long-range dependencies and complex patterns in language.

Both Sequence-to-Sequence models are based on T5 models \cite{t5}, which employ an encoder-decoder architecture: while an encoder processed the input sequence into a context vector, and an decoder generates an input sequence from this vector.
The \texttt{Flan-T5} models are fine-tuned to follow instructions, and have improved zero-shot performance compared to the original T5 models \cite{flant5}.

\texttt{Flan-T5-XL} contains approximately 3 billion parameters.
This is considerably bigger than the base Flan-T5 model \cite{flant5}, which will provide better accuracy of its parametric answers.

\texttt{Flan-T5-XXL} contains 11 billion parameters, has higher accuracy on the parametric answers as the \texttt{XL} model \cite{flant5}.
However, how the higher amount of parameters will affect its knowledge grounding when running our experiment is still unknown.

Decoder-only models generate answers one token at a time from the input query.
Given a sequence of tokens, they generate text one token at a time by attempting to solve the problem of predicting the following token \cite{gpt}.

This thesis uses the \texttt{-Instruct} versions of the latest Llama models \cite{llama3}, which use this architecture and fine-tune it to tasks of instruction-following.
These models are specially adept at complex prompts.
Of the models used in this thesis, \texttt{Meta-Llama-3.1-8B-Instruct} has 8 billion parameters, while \texttt{Meta-Llama-3.1-70B-Instruct} has 70 billion.

% The properties of the models are summarised in \cref{model_card}.
% 
% \begin{table}[htbp]
% 	\centering
% 	\footnotesize
% 	\begin{tabular}{>{\ttfamily}l@{\hspace{3pt}}l@{\hspace{3pt}}l@{\hspace{3pt}}r@{\hspace{3pt}}r}
% 		\toprule
% 			\rmfamily \bfseries Model & \bfseries Architecture & \bfseries Trained On & \bfseries Parameters \\
% 		\midrule
% 			Flan-T5-XL & Seq2Seq & Public NLP Datasets & 3 Billion \\
% 			Flan-T5-XXL & Seq2Seq & Public NLP Datasets & 11 Billion \\[2pt]
% 			Meta-Llama-3.1-8B-Instruct & Decoder-Only & Web text, code, instruction datasets & 8 Billion \\
% 			Meta-Llama-3.1-70B-Instruct & Decoder-Only & Web text, code, instruction datasets & 70 Billion \\
% 		\bottomrule
% 	\end{tabular}
% 	\caption{Model cards of the large language models used in this research.}
% 	\label{model_card}
% \end{table}

\subsection{Understanding the source of the answer in each model}
\label{query_design}

\input{counterparametric_example}

The first step to understanding the knowledge grounding of large language models is to create queries that contain data that contradicts its parametric knowledge as part of the context.
By comparing the result to the existing answers it becomes trivial to understand whether an answer came from the model's memory, the queries' context, or neither of these.

Following the approach done by Yu et al.\ \cite{factual_recall}, for every query we randomly sample from the set of answers of the same base question for answers that are different to the parametric answer which is given by the original query.
% An example of this random sampling can be found in \cref{counterparametric_table}.

We later add this \emph{counterparametric answer} to the context, to form a new query and query the same model again with the added counterparametric context.
This is exemplified in \cref{counterparametric_example}.

To ensure that the results are simple to interpret and minimise the effect of randomness, once we select the queries we follow the example of Hsia et al.\ \cite{ragged} and use Greedy Decoding to generate the answer.
While beam search with tends to produce more accurate results for long answers \cite{sutskever_seq2seqlearning,wu_mltranslation} and there are many other sampling methods that tend to produce better results \cite{text_degeneration}, this is likely to not have an effect on experiments shorter answers \cite{t5}.

We compare the generated answer with the context to the previously generated parametric answer, and we categorise the answer:
\begin{description}
	\item[\Parametric{}] answers are equal to the answer given by the model when queried without context.
		This answer would come from the parametric memory of the model, and could potentially indicate an hallucination not present in the context.
	\item[\Contextual{}] answers are equal to the context given in the query.
		In a RAG context, this would be the answer retrieved from the index.
	\item[\Other] answers are neither of these, and this answer comes from a mis-interpretation of the input by the model or from some other source.
\end{description}

To minimise the amount of problems caused by large language models generating extra information, we compare answers by truncating the text until the first period or \texttt{<EOS>} token, removing punctuation and stop words, and finding whether one of the answers is a subsequence of another.
