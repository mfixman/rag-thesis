\section{Methods}
\label{methodology_section}

This study investigates the behavior of large language models (LLMs) when presented with context that contradicts their parametric, learned knowledge.
To achieve this, we develop a framework for evaluating the knowledge grounding of LLMs across different architectures and model sizes.

\subsection{Dataset Creation}
\label{dataset_creation}

\subsubsection{Rationale and comparison to prior datasets}

The foundation of this work is a representative dataset of questions designed to test the interplay between parametric and contextual knowledge in LLMs.
This dataset must satisfy three properties:
\begin{description}[style=nextline]
	\item[1. Short, unambiguous answers] Questions must be constructed to elicit concise answers, enabling precise comparison and interpretation.
This avoids ambiguity and minimizes variability in answers, which is critical for identifying parametric versus contextual sources.
	\item[2. Coverage of diverse topics] The dataset must span a wide range of domains, from historical events to scientific concepts, to mitigate biases inherent in training data~\cite{wikipedia_geographic_bias}.
	This diversity ensures a robust evaluation of grounding across different knowledge areas.
	\item[3. Conterparametric compatibility] Questions are designed to facilitate the addition of a context allowing an answer that contradicts the parametric answer.
		An answer different to the parametric answer must be incorrect.
\end{description}

There exists a variety of datasets that can be used for similar research, which are explored in \cref{related_work}.
However, none of those are suited for this research.
These limitations motivated the creation of a custom dataset.

\subsubsection{Dataset Design and Generation}

The design of this dataset is inspired by the methodology designed by Yu et al.~\cite{factual_recall}.
Several queries of the form ``\texttt{What is the capital of {country}?}'' are asked for a large list of countries.
Later, these parametric answers are used as counterparametric answers for questions relating to different countries.

This paper creates a similar but larger and more varied dataset of questions and answers from a wide range of topics.
We can then emulate the approach used in that paper of reusing the answer from a certain question as the counterfactual context of another.

Our dataset consists of 9 different categories, each of which has a series of manually-written questions that can be answered with short and simple answers.
To ensure diversity and representativeness, we manually crafted 100 base questions and 411 objects across these categories.
By combining each base question with the corresponding objects, we generated a total of 4760 unique questions.
The categories and their respective breakdown are as follows:

\begin{enumerate}
	\item \textbf{Person}: Historical figures from early antiquity to the present day, spanning all regions of the globe.
	\item \textbf{City}: Cities worldwide, with questions covering population, founding dates, notable landmarks, or geographical features.
	\item \textbf{Principle}: Scientific principles discovered from the 16th century onward.
	\item \textbf{Element}: Elements from the periodic table.
	\item \textbf{Book}: Literary works from various genres, time periods, and cultures.
	\item \textbf{Painting}: Famous artworks from different art movements and periods.
	\item \textbf{Historical Event}: Significant occurrences that shaped world history, from ancient times to the modern era.
	\item \textbf{Building}: Notable structures worldwide, including ancient monuments, modern skyscrapers, and architectural wonders.
	\item \textbf{Composition}: Musical works from various genres and time periods.
\end{enumerate}

Each category's base questions were systematically paired with the corresponding objects, following and extending the question-building approach used by Yu et al.~\cite{factual_recall}.
The total number of questions per category, along with the breakdown of base questions and objects, is detailed in \cref{category_amounts}.
The full set of questions, along with the code used in this study, is available in the accompanying repository.

\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{>{\bfseries}l r r r}
		\toprule
			\bfseries Category & \bfseries Base Questions & \bfseries Objects & \bfseries Total Questions \\
		\midrule
			Person & 17 & 57 & 969 \\
			City & 17 & 70 & 1190 \\
			Principle & 5 & 37 & 185 \\
			Element & 15 & 43 & 645 \\
			Book & 11 & 49 & 539 \\
			Painting & 12 & 44 & 528 \\
			Historical Event & 4 & 64 & 256 \\
			Building & 9 & 22 & 198 \\
			Composition & 10 & 25 & 250 \\
		\midrule
			Total & 100 & 411 & 4760 \\
		\bottomrule \addlinespace[4pt]
	\end{tabular}
	\caption{The number of base questions, objects, and total questions in each category of the final dataset. The totals are derived by combining each base question with the objects in its respective category.}
	\label{category_amounts}
\end{table}

\subsection{Query Design}
\label{query_design}

The first step to understanding the knowledge grounding of large language models is to create queries that contain data that contradicts its parametric knowledge as part of the context.
By comparing the result to the existing answers it becomes possible to understand whether an answer came from the model's memory, the queries' context, or neither of these.

We follow the approach by Yu et al.~\cite{factual_recall}: to test the knowledge grounding of each large language model, for every question generated with the procedure described in the previous subsection, we randomly sample an answer from the set of answers of the same base question for answers that are different to the parametric answer which is given by the original query.
This ensures that this answer is different to the parametric answer to the question.

We refer to this answer as the \emph{counterparametric answer}.

This later is later concatenated to new prompt which uses the same question to form a new query and query the same model again with the added counterparametric context.
This process is exemplified in \cref{counterparametric_example}.

\subsection{Query execution and categorisation of answers}

To ensure that the results are simple to interpret and minimise the effect of randomness, we follow the example of Hsia et al.~\cite{ragged} and use greedy decoding to generate the answer.

We compare the generated answer with the context to the previously generated parametric answer, and we categorise the answer into one of three categories depending on its equality to the possible answers.

\begin{description}
	\item[\Parametric{}] answers are equal to the answer given by the model when queried without context.
		This answer would come from the parametric memory of the model, and could potentially indicate an hallucination not present in the context.
	\item[\Contextual{}] answers are equal to the context given in the query.
		When using a context generated by RAG, this answer would be retrieved from the index.
	\item[\Other] answers are neither of these, and this answer comes from a mis-interpretation of the input by the model or from some other source.
\end{description}

To minimise the amount of problems caused by large language models generating extra information, we truncate answers on the first period or \texttt{<EOS>} token and remove punctuation and stop words.

