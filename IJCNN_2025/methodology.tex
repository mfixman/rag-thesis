\section{Methods}
\label{methodology_section}
% \todo{Should this be ``Methods'' or ``Methodology''?}


This study investigates the behavior of large language models (LLMs) when presented with context that contradicts their parametric, learned knowledge.
To achieve this, we develop a comprehensive framework for evaluating the knowledge grounding of LLMs across different architectures and model sizes.

\subsection{Dataset Creation}
\label{dataset_creation}

\subsubsection{Rationale and comparison to prior datasets}

The foundation of this work is a representative dataset of questions designed to test the interplay between parametric and contextual knowledge in LLMs.
This dataset must satisfy three properties:
\begin{description}[style=nextline]
	\item[Short, unambiguous answers] Questions are constructed to elicit concise answers, enabling precise comparison and interpretation.
This avoids ambiguity and minimizes variability in answers, which is critical for identifying parametric versus contextual sources.
	\item[Coverage of diverse topics] The dataset spans a wide range of domains, from historical events to scientific concepts, to mitigate biases inherent in training data \cite{wikipedia_geographic_bias}.
	This diversity ensures a robust evaluation of grounding across different knowledge areas.
	\item[Conterparametric compatibility] Questions are designed to facilitate the addition of a context allowing an answer that contradicts the parametric answer.
\end{description}

Existing datasets, such as the Natural Questions dataset \cite{natural_questions} and the Countries' Capitals dataset \cite{factual_recall}, provided valuable insights but fell short of meeting all three criteria.
For example, while the Natural Questions dataset offers a wide range of questions, its lack of systematic categorization hinders counterparametric experiments.
The Countries' Capitals dataset, while well-suited for counterparametric evaluation, is limited in scope.

These limitations motivated the creation of a custom dataset.

\subsubsection{Dataset Design and Generation}

The design of this dataset is inspired by the methodology designed by Yu et al.\ \cite{factual_recall}.
In this paper, several queries of the form ``\texttt{What is the capital of \{country\}?}'' are asked and answers from different countries are used as counterfactual information.

This paper creates a similar but larger and more varied dataset of questions and answers from a wide range of topics, assuring questions can be grouped by question pattern so that the formats of their answer are similar.
This way we can emulate the approach used in that paper of reusing the answer from a certain question as the counterfactual context of another.

Our dataset consists of 9 different categories, each of which has a series of manually-written questions that can be answered with short and simple answers.

\subsection{Model Selection}
\label{model_selection}

In order to understand the knowledge grounding of a wide variety of large language
models, the queries in the dataset we previously generated are tested into models of various architectures and sizes, which are listed in \cref{model_list}.

\begin{table}[htb]
	\centering
	\footnotesize
	% \renewcommand{\arraystretch}{1.2}
	\begin{tabular}{l l r}
		\toprule
			Model             & Architecture    & Params \\
		\midrule
			\smallflan{}      & Encoder-Decoder & 3B          \\
			\bigflan{}        & Encoder-Decoder & 11B         \\
			\smallllama{}    & Decoder-Only    & 8B          \\
			\bigllama{} & Decoder-Only    & 70B         \\
		\bottomrule \addlinespace[4pt]
	\end{tabular}
	\caption{Models evaluated in this study.}
	\label{model_list}
\end{table}

% Putting this table here to ensure it's at the top of the previous page.
\input{counterparametric_example}

All of the models used in this research leverage autoregressive attention using the transformer architecture \cite{attention_is_all_you_need}, where each token attends to its preceding tokens, maintaining the temporal order of the sequence.
This approach allows them to generate coherent and contextually relevant text by sampling from this learned distribution, while also capturing long-range dependencies and complex patterns in language.

Both Seq2Seq models are based on T5 models \cite{t5}, which employ an encoder-decoder architecture: while an encoder processed the input sequence into a context vector, and an decoder generates an input sequence from this vector.
The \texttt{Flan-T5} models are fine-tuned to follow instructions, and have improved zero-shot performance compared to the original T5 models \cite{flant5}.

\texttt{Flan-T5-XL} contains approximately 3 billion parameters.
This is considerably bigger than the base Flan-T5 model \cite{flant5}, which will provide better accuracy of its parametric answers.

\texttt{Flan-T5-XXL} contains 11 billion parameters, has higher accuracy on the parametric answers as the \texttt{XL} model \cite{flant5}.

Decoder-only models generate answers one token at a time from the input query.
Given a sequence of tokens, they generate text one token at a time by attempting to solve the problem of predicting the following token \cite{gpt}.

This paper uses the \texttt{-Instruct} versions of the latest Llama models \cite{llama3}, which use this architecture and fine-tune it to tasks of instruction-following.
These models are specially adept at complex prompts.
Of the models used in this paper, \texttt{Meta-Llama-3.1-8B-Instruct} has 8 billion parameters, while \texttt{Meta-Llama-3.1-70B-Instruct} has 70 billion.

\subsection{Understanding the source of the answer in each model}
\label{query_design}

The first step to understanding the knowledge grounding of large language models is to create queries that contain data that contradicts its parametric knowledge as part of the context.
By comparing the result to the existing answers it becomes trivial to understand whether an answer came from the model's memory, the queries' context, or neither of these.

Following the approach done by Yu et al.\ \cite{factual_recall}, for every query we randomly sample from the set of answers of the same base question for answers that are different to the parametric answer which is given by the original query.
% An example of this random sampling can be found in \cref{counterparametric_table}.

We later add this \emph{counterparametric answer} to the context, to form a new query and query the same model again with the added counterparametric context.
This is exemplified in \cref{counterparametric_example}.

To ensure that the results are simple to interpret and minimise the effect of randomness, once we select the queries we follow the example of Hsia et al.\ \cite{ragged} and use Greedy Decoding to generate the answer.
While beam search with tends to produce more accurate results for long answers \cite{sutskever_seq2seqlearning,wu_mltranslation} and there are many other sampling methods that tend to produce better results \cite{text_degeneration}, this is likely to not have an effect on experiments shorter answers \cite{t5}.

We compare the generated answer with the context to the previously generated parametric answer, and we categorise the answer into one of three categories depending on its equality to the possible answers.
\begin{description}
	\item[\Parametric{}] answers are equal to the answer given by the model when queried without context.
		This answer would come from the parametric memory of the model, and could potentially indicate an hallucination not present in the context.
	\item[\Contextual{}] answers are equal to the context given in the query.
		In a RAG context, this would be the answer retrieved from the index.
	\item[\Other] answers are neither of these, and this answer comes from a mis-interpretation of the input by the model or from some other source.
\end{description}

To minimise the amount of problems caused by large language models generating extra information, we compare answers by truncating the text until the first period or \texttt{<EOS>} token, removing punctuation and stop words, and finding whether one of the answers is a subsequence of another.

